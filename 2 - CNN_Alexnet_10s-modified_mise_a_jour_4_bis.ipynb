{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK : CNN_Alexnet_cutting=10s for bulls detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Code for big plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval of audios and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make panda dataframe with paths and labels of audio files\n",
    "\n",
    "def getAudioFiles(path_directory):\n",
    "        \n",
    "        files_path = []\n",
    "        files_name = []\n",
    "\n",
    "        for path, subdirs, files in os.walk(path_directory):\n",
    "            files.sort()\n",
    "            for name in files :\n",
    "                if name.endswith(\".wav\"):\n",
    "                    files_path.append(path + os.path.sep + name)\n",
    "        #print(\"-\", len(files_path), \"files found in the directory\", path_directory,'\\n')\n",
    "\n",
    "        return files_path\n",
    "    \n",
    "\n",
    "#folder_audio = \"C:/Users/thomas guerin/Documents/COURS IMT MINES ALES/Département 2IA/3A/S10/Etude technique/DATA/lgi2p-alose/tmp/bulls_audio/audio_out\"\n",
    "folder_audio = \"/mnt/Baie-MD1400/data/guyot/dev/MRM/lgi2p-alose/tmp/bulls_audio/audio_out\"\n",
    "\n",
    "\n",
    "\n",
    "files_path = getAudioFiles(folder_audio)\n",
    "number = len(files_path)\n",
    "\n",
    "filenames = []\n",
    "dic_Labels = {}\n",
    "#pathOfLabels = \"C:/Users/thomas guerin/Documents/COURS IMT MINES ALES/Département 2IA/3A/S10/Etude technique/DATA/lgi2p-alose/tmp/bulls_audio/audio_annotated\"\n",
    "pathOfLabels = \"/mnt/Baie-MD1400/data/guyot/dev/MRM/lgi2p-alose/tmp/bulls_audio/audio_annotated\"\n",
    "\n",
    "for path, subdirs, files in os.walk(pathOfLabels):\n",
    "    for filename in sorted(files) :\n",
    "        with open(os.path.join(pathOfLabels, filename),'r') as labels_file:\n",
    "            lines = labels_file.readlines()\n",
    "            labels_file.close()\n",
    "            for line in lines :\n",
    "                split_line = line.split(' ')\n",
    "                filenames.append(line)\n",
    "                file = split_line[0]\n",
    "                file_name, extension = os.path.splitext(file)\n",
    "                dic_Labels[file_name] = split_line[1][0]\n",
    "                \n",
    "y = np.zeros(number)\n",
    "for idx_file, file_path in enumerate(files_path):\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    y[idx_file] = int(dic_Labels[file_name])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of dataloaders (for the use of data \"on the fly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, files, labels, transform):\n",
    "    super(MyDataset,self).__init__()\n",
    "    self.files = files_path\n",
    "    self.labels = labels\n",
    "    self.transform = transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.files)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    wave = torchaudio.load(self.files[idx])[0] # Il faudrait déterminer les paramètres de transformation\n",
    "    melspectro = self.transform(wave)\n",
    "    label = self.labels[idx]\n",
    "    return melspectro,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_indexes(text):\n",
    "    l = []\n",
    "    n = len(files_path)\n",
    "    for i in range(n):\n",
    "        if text in files_path[i]:\n",
    "            l.append(i)\n",
    "    return [l[0],l[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for validation and test\n",
    "\n",
    "index_ceze_2017_p1 = return_indexes(text = \"2017_05_22-23_13_00\")\n",
    "\n",
    "index_ceze_2017_p2 = return_indexes(text = \"2017_05_24-23_15_00\")\n",
    "\n",
    "index_ceze_2017_p3 = return_indexes(text = \"2017_05_26-23_03_00\")\n",
    "\n",
    "index_ceze_2017_p4 = return_indexes(text = \"2017_06_01-23_20_00\")\n",
    "\n",
    "index_ceze_2018 = return_indexes(text = \"2018-06-10_22-25\")\n",
    "\n",
    "index_ardoise_2014_p1 = return_indexes(text = \"2014_05_24-01_41_10\")\n",
    "\n",
    "index_ardoise_2014_p2 = return_indexes(text = \"2014_05_24-23_14_00\")\n",
    "\n",
    "\n",
    "validation_index = list(range(index_ardoise_2014_p2[0]+869,index_ardoise_2014_p2[1]+1)) + list(range(index_ceze_2017_p1[0],index_ceze_2017_p1[1]+1)) + list(range(index_ceze_2017_p2[0],index_ceze_2017_p2[1]+1)) + list(range(index_ceze_2017_p3[0],index_ceze_2017_p3[1]+1)) \n",
    "test_index = list(range(index_ardoise_2014_p1[0],index_ardoise_2014_p1[1]+1)) + list(range(index_ardoise_2014_p2[0],index_ardoise_2014_p2[0]+869)) + list(range(index_ceze_2017_p4[0],index_ceze_2017_p4[1]+1)) + list(range(index_ceze_2018[0],index_ceze_2018[1]+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_importation(shuffle=False):\n",
    "    \n",
    "    # Transforming audio files into melspectrograms\n",
    "    transform = transforms.Compose([\n",
    "        torchaudio.transforms.MelSpectrogram(n_fft=4096, hop_length=4096//4),\n",
    "        transforms.Resize((128,431))\n",
    "    ]) \n",
    "    \n",
    "    df = MyDataset(files_path, y, transform)\n",
    "    \n",
    "    validation_indexes = validation_index\n",
    "    test_indexes = test_index\n",
    "    train_indexes = list( set(range(len(df))) - set(validation_indexes) - set(test_indexes) )\n",
    "    \n",
    "\n",
    "    train_set = torch.utils.data.Subset(df, train_indexes)\n",
    "    validation_set = torch.utils.data.Subset(df, validation_indexes)\n",
    "    test_set = torch.utils.data.Subset(df, test_indexes)\n",
    "    \n",
    "    trainloader = DataLoader(train_set, batch_size=16, shuffle=shuffle, num_workers=0)\n",
    "    validationloader = DataLoader(validation_set, batch_size=16, shuffle=False, num_workers=0)\n",
    "    testloader = DataLoader(test_set, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    bulls_proportion = (list(y[:train_indexes[-1]]).count(1))/len(train_indexes)\n",
    "    weight_bulls = 1 / (bulls_proportion + 1)\n",
    "    weight_no_bulls = 1 / (( 1 / (bulls_proportion + 0.0001) ) + 1)\n",
    "    # Those weights will be used to manage the problem of unbalanced classes ( nb(no_bulls)>>nb(bulls) )\n",
    "    \n",
    "    return trainloader,validationloader,testloader,weight_bulls,weight_no_bulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_alexnet(torch.nn.Module):\n",
    "    def __init__(self, pretrained=False, freeze=False):\n",
    "        super(model_alexnet, self).__init__()\n",
    "        alexnet = models.alexnet(pretrained=pretrained)\n",
    "        if freeze:\n",
    "            alexnet = models.alexnet(pretrained=True)\n",
    "            freeze_model(alexnet)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(num_features=1, affine=False)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 3, (1,1), stride=(1,1), padding=0, bias=False)\n",
    "        self.alexnet = alexnet\n",
    "        self.drop1 = torch.nn.Dropout(0.3)\n",
    "        self.lin1 = torch.nn.Linear(in_features=1000, out_features=100, bias=True)\n",
    "        self.drop2 = torch.nn.Dropout(0.2)\n",
    "        self.lin2 = torch.nn.Linear(in_features=100, out_features=2, bias=True)\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.alexnet(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, name_model, device, epochs, learning_rate=1e-4, batch_size=128, early_stop_nb = 10):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    weight = torch.tensor([weight_no_bulls,weight_bulls])\n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean',weight=weight)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #bulls = torch.tensor([1 for i in range(16)])\n",
    "    #no_bulls = torch.tensor([0 for i in range(16)])\n",
    "    #bulls = bulls.to(device)\n",
    "    #no_bulls = no_bulls.to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    epoch_improvement = [1]\n",
    "    \n",
    "    list_train_loss_cumul = []\n",
    "    list_metric = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        \n",
    "        model.train() # we specify that we are training the model\n",
    "        \n",
    "        # At each epoch, the training set will be processed as a set of batches\n",
    "        \n",
    "        # We use accumulated gradients ( ex: batch_size = 128 = 8 * (16 batches) ) to avoid memory pb\n",
    "        \n",
    "        accum_step = batch_size//16\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss_cumul = 0\n",
    "        \n",
    "        for batch_id,  batch in enumerate(trainloader) : \n",
    "            \n",
    "            images, labels  = batch\n",
    "            \n",
    "            labels = labels.long()\n",
    "            \n",
    "            bulls = torch.tensor([1 for i in range(labels.shape[0])])\n",
    "            no_bulls = torch.tensor([0 for i in range(labels.shape[0])])\n",
    "            bulls = bulls.to(device)\n",
    "            no_bulls = no_bulls.to(device)\n",
    "            \n",
    "            # we put the data on the same device\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            y_pred = model(images) # forward pass output=logits\n",
    "            \n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            \n",
    "            loss /= accum_step  # Because the cross entropy loss is a mean\n",
    "            \n",
    "            train_loss_cumul += loss.item()\n",
    "            \n",
    "            loss.backward()       # update the gradient (cf https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa#:~:text=Gradient%20accumulation%20means%20running%20a,Yes%2C%20it's%20really%20that%20simple.)\n",
    "            \n",
    "            if ( (batch_id + 1) % accum_step == 0 ):\n",
    "                \n",
    "                optimizer.step() # update the model parameters using the gradient\n",
    "                \n",
    "                optimizer.zero_grad() # clear the gradient before backward\n",
    "                \n",
    "                print(\"epoch: {:03d}, batch: {:03d}, loss: {:.3f} \".format(t+1, (batch_id+1)//accum_step, loss.item()*accum_step))\n",
    "                #print(\"e: {:03d}, b: {:03d}, l: {:.3f} \".format(t+1, (batch_id+1)//accum_step, loss.item()*accum_step), end='')\n",
    "                \n",
    "                \n",
    "        # Training score\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_real_bulls = 0\n",
    "        total_real_no_bulls = 0\n",
    "        total_predicted_bulls = 0\n",
    "        total_predicted_no_bulls = 0\n",
    "        correct_real_bulls = 0\n",
    "        correct_real_no_bulls = 0\n",
    "        correct_predicted_bulls = 0\n",
    "        correct_predicted_no_bulls = 0\n",
    "        \n",
    "        for batch_id, batch in enumerate(trainloader):\n",
    "           \n",
    "            images , labels = batch\n",
    "                        \n",
    "            labels = labels.long() # float --> Long\n",
    "            \n",
    "            bulls = torch.tensor([1 for i in range(labels.shape[0])])\n",
    "            no_bulls = torch.tensor([0 for i in range(labels.shape[0])])\n",
    "            bulls = bulls.to(device)\n",
    "            no_bulls = no_bulls.to(device)\n",
    "              \n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            y_pred = model(images) # forward computes the logits\n",
    "            \n",
    "            sf_y_pred = torch.nn.Softmax(dim=1)(y_pred) # softmax to obtain the probability distribution\n",
    "            \n",
    "            _, predicted = torch.max(sf_y_pred , 1)     # decision rule, we select the max\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_real_bulls += (labels == bulls).sum().item()\n",
    "            total_real_no_bulls += (labels == no_bulls).sum().item()\n",
    "            total_predicted_bulls += (predicted == bulls).sum().item()\n",
    "            total_predicted_no_bulls += (predicted == no_bulls).sum().item()\n",
    "            correct_real_bulls += ( (predicted == labels) & (labels == bulls) ).sum().item()\n",
    "            correct_real_no_bulls += ( (predicted == labels) & (labels == no_bulls) ).sum().item()\n",
    "            correct_predicted_bulls += ( (predicted == labels) & (predicted == bulls) ).sum().item()\n",
    "            correct_predicted_no_bulls += ( (predicted == labels) & (predicted == no_bulls) ).sum().item()\n",
    "            \n",
    "        accuracy = correct / total\n",
    "            \n",
    "        bulls_recall = correct_real_bulls / (total_real_bulls + 0.01)\n",
    "        bulls_precision = correct_predicted_bulls / (total_predicted_bulls + 0.01)\n",
    "            \n",
    "        no_bulls_recall = correct_real_no_bulls / (total_real_no_bulls + 0.01)\n",
    "        no_bulls_precision = correct_predicted_no_bulls / (total_predicted_no_bulls + 0.01)\n",
    "        \n",
    "        list_train_loss_cumul.append(train_loss_cumul)\n",
    "            \n",
    "        print(\"epoch: {:03d} ------------------------------------------------\".format(t+1)+ \"\\n\")\n",
    "        print(\"[train] loss: {:.3f}\\n\".format(train_loss_cumul))\n",
    "        print(\"\\n\" + \"[train] accuracy: {:.3f}%\\n\".format(100*accuracy) )\n",
    "        print(\"[train] bulls_recall: {:.3f}%\\n\".format(100*bulls_recall))\n",
    "        print(\"[train] bulls_precision: {:.3f}%\\n\".format(100*bulls_precision) )\n",
    "        print(\"[train] no_bulls_recall: {:.3f}%\\n\".format(100*no_bulls_recall))\n",
    "        print(\"[train] no_bulls_precision: {:.3f}%\\n\".format(100*no_bulls_precision) )\n",
    "        \n",
    "        \n",
    "        # Validation score\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_real_bulls = 0\n",
    "        total_real_no_bulls = 0\n",
    "        total_predicted_bulls = 0\n",
    "        total_predicted_no_bulls = 0\n",
    "        correct_real_bulls = 0\n",
    "        correct_real_no_bulls = 0\n",
    "        correct_predicted_bulls = 0\n",
    "        correct_predicted_no_bulls = 0\n",
    "        \n",
    "        for batch_id, batch in enumerate(validationloader):\n",
    "            \n",
    "            images , labels = batch\n",
    "            \n",
    "            labels = labels.long() # float --> Long\n",
    "            \n",
    "            bulls = torch.tensor([1 for i in range(labels.shape[0])])\n",
    "            no_bulls = torch.tensor([0 for i in range(labels.shape[0])])\n",
    "            bulls = bulls.to(device)\n",
    "            no_bulls = no_bulls.to(device)\n",
    "            \n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            y_pred = model(images) # forward computes the logits\n",
    "            sf_y_pred = torch.nn.Softmax(dim=1)(y_pred) # softmax to obtain the probability distribution\n",
    "            _, predicted = torch.max(sf_y_pred , 1)     # decision rule, we select the max\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_real_bulls += (labels == bulls).sum().item()\n",
    "            total_real_no_bulls += (labels == no_bulls).sum().item()\n",
    "            total_predicted_bulls += (predicted == bulls).sum().item()\n",
    "            total_predicted_no_bulls += (predicted == no_bulls).sum().item()\n",
    "            correct_real_bulls += ( (predicted == labels) & (labels == bulls) ).sum().item()\n",
    "            correct_real_no_bulls += ( (predicted == labels) & (labels == no_bulls) ).sum().item()\n",
    "            correct_predicted_bulls += ( (predicted == labels) & (predicted == bulls) ).sum().item()\n",
    "            correct_predicted_no_bulls += ( (predicted == labels) & (predicted == no_bulls) ).sum().item()\n",
    "            \n",
    "        accuracy = correct / total\n",
    "            \n",
    "        bulls_recall = correct_real_bulls / (total_real_bulls + 0.01)\n",
    "        bulls_precision = correct_predicted_bulls / (total_predicted_bulls + 0.01)\n",
    "            \n",
    "        no_bulls_recall = correct_real_no_bulls / (total_real_no_bulls + 0.01)\n",
    "        no_bulls_precision = correct_predicted_no_bulls / (total_predicted_no_bulls + 0.01)\n",
    "        \n",
    "        metric = (bulls_recall + no_bulls_recall)/2\n",
    "        \n",
    "        list_metric.append(metric)\n",
    "            \n",
    "        print(\"[validation] accuracy: {:.3f}%\\n\".format(100*accuracy))\n",
    "        print(\"[validation] bulls_recall: {:.3f}%\\n\".format(100*bulls_recall))\n",
    "        print(\"[validation] bulls_precision: {:.3f}%\\n\".format(100*bulls_precision))\n",
    "        print(\"[validation] no_bulls_recall: {:.3f}%\\n\".format(100*no_bulls_recall))\n",
    "        print(\"[validation] no_bulls_precision: {:.3f}%\\n\".format(100*no_bulls_precision))\n",
    "        print(\"[validation] scoring metric (average recall): \" + str(metric) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        # Early Stopping\n",
    "        \n",
    "        val_score = metric\n",
    "        \n",
    "        if val_score >= best_score:\n",
    "            best_score = val_score\n",
    "            early_stop_count = 0\n",
    "            path = name_model + \"__epoch\" + str(epoch_improvement[-1]) + \".pth\"\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path)\n",
    "            epoch_improvement.append(t+1)\n",
    "            new_path = name_model + \"__epoch\" + str(epoch_improvement[-1]) + \".pth\"\n",
    "            torch.save(model, new_path)\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count == early_stop_nb:\n",
    "            break\n",
    "        \n",
    "    print(\"\\n\" + \"Final nb epochs : \" + str(epoch_improvement[-1]))\n",
    "    print(\"Best validation score (= best bulls_fscore) : \" + str(best_score))\n",
    "    \n",
    "    \n",
    "    # We plot graphs for training and validation errors\n",
    "    \n",
    "    num_epochs = list(range(1,t+2))\n",
    "    \n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"training loss\")\n",
    "    plt.plot(num_epochs, list_train_loss_cumul)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric (on validation dataset)\")\n",
    "    plt.plot(num_epochs, list_metric)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # We return the best model\n",
    "    \n",
    "    model = torch.load(new_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_real_bulls = 0\n",
    "    total_real_no_bulls = 0\n",
    "    total_predicted_bulls = 0\n",
    "    total_predicted_no_bulls = 0\n",
    "    correct_bulls = 0\n",
    "    correct_no_bulls = 0\n",
    "    \n",
    "    for batch_id, batch in enumerate(testloader):\n",
    "        \n",
    "        images , labels = batch\n",
    "        \n",
    "        bulls = torch.tensor([1 for i in range(labels.shape[0])])\n",
    "        no_bulls = torch.tensor([0 for i in range(labels.shape[0])])\n",
    "        \n",
    "        bulls = bulls.to(device)\n",
    "        no_bulls = no_bulls.to(device)\n",
    "        \n",
    "        labels = labels.long() # float --> Long\n",
    "        \n",
    "        images , labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        y_pred = model(images) # forward computes the logits\n",
    "        sf_y_pred = torch.nn.Softmax(dim=1)(y_pred) # softmax to obtain the probability distribution\n",
    "        _, predicted = torch.max(sf_y_pred , 1)     # decision rule, we select the max\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total_real_bulls += (labels == bulls).sum().item()\n",
    "        total_real_no_bulls += (labels == no_bulls).sum().item()\n",
    "        total_predicted_bulls += (predicted == bulls).sum().item()\n",
    "        total_predicted_no_bulls += (predicted == no_bulls).sum().item()\n",
    "        correct_bulls += ( (predicted == labels) & (labels == bulls) ).sum().item()\n",
    "        correct_no_bulls += ( (predicted == labels) & (labels == no_bulls) ).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    bulls_recall = correct_bulls / (total_real_bulls + 0.01)\n",
    "    bulls_precision = correct_bulls / (total_predicted_bulls + 0.01)\n",
    "    \n",
    "    no_bulls_recall = correct_no_bulls / (total_real_no_bulls + 0.01)\n",
    "    no_bulls_precision = correct_no_bulls / (total_predicted_no_bulls + 0.01)\n",
    "    \n",
    "    metric = (bulls_recall + no_bulls_recall)/2\n",
    "    \n",
    "    print(\"Confusion matrix (rows : real (no_bulls and bulls)  ;  columns : predicted (same) ): \")\n",
    "    print(str(correct_bulls) + \"  |  \" + str(total_real_bulls - correct_bulls))\n",
    "    print(str(total_real_no_bulls - correct_no_bulls) + \"  |  \" + str(correct_no_bulls) + \"\\n\")\n",
    "    print(\"[test] accuracy: {:.3f}%\\n\".format(100*accuracy) + \"\\n\")\n",
    "    print(\"[test] bulls_recall: {:.3f}%\\n\".format(100*bulls_recall))\n",
    "    print(\"[test] bulls_precision: {:.3f}%\\n\".format(100*bulls_precision) + \"\\n\")\n",
    "    print(\"[test] no_bulls_recall: {:.3f}%\\n\".format(100*no_bulls_recall))\n",
    "    print(\"[test] no_bulls_precision: {:.3f}%\\n\".format(100*no_bulls_precision) + \"\\n\")\n",
    "    print(\"[test] scoring metric (average recall): \" + str(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guyot/miniconda3/envs/my_env/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/home/guyot/miniconda3/envs/my_env/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001, batch: 001, loss: 0.786 \n",
      "epoch: 001, batch: 002, loss: 0.795 \n",
      "epoch: 001, batch: 003, loss: 0.707 \n",
      "epoch: 001, batch: 004, loss: 0.225 \n",
      "epoch: 001, batch: 005, loss: 0.223 \n",
      "epoch: 001, batch: 006, loss: 1.485 \n",
      "epoch: 001, batch: 007, loss: 1.081 \n",
      "epoch: 001, batch: 008, loss: 1.102 \n",
      "epoch: 001, batch: 009, loss: 0.301 \n",
      "epoch: 001, batch: 010, loss: 0.821 \n",
      "epoch: 001, batch: 011, loss: 0.304 \n",
      "epoch: 001, batch: 012, loss: 0.284 \n",
      "epoch: 001, batch: 013, loss: 0.624 \n",
      "epoch: 001, batch: 014, loss: 0.676 \n",
      "epoch: 001, batch: 015, loss: 0.249 \n",
      "epoch: 001, batch: 016, loss: 0.304 \n",
      "epoch: 001, batch: 017, loss: 0.253 \n",
      "epoch: 001, batch: 018, loss: 0.552 \n",
      "epoch: 001, batch: 019, loss: 1.177 \n",
      "epoch: 001, batch: 020, loss: 0.766 \n",
      "epoch: 001, batch: 021, loss: 0.330 \n",
      "epoch: 001, batch: 022, loss: 0.876 \n",
      "epoch: 001, batch: 023, loss: 0.143 \n",
      "epoch: 001, batch: 024, loss: 0.252 \n",
      "epoch: 001, batch: 025, loss: 0.203 \n",
      "epoch: 001, batch: 026, loss: 0.829 \n",
      "epoch: 001, batch: 027, loss: 0.074 \n",
      "epoch: 001, batch: 028, loss: 0.830 \n",
      "epoch: 001, batch: 029, loss: 0.741 \n",
      "epoch: 001, batch: 030, loss: 1.049 \n",
      "epoch: 001, batch: 031, loss: 0.380 \n",
      "epoch: 001, batch: 032, loss: 0.656 \n",
      "epoch: 001, batch: 033, loss: 0.770 \n",
      "epoch: 001, batch: 034, loss: 0.899 \n",
      "epoch: 001, batch: 035, loss: 0.857 \n",
      "epoch: 001, batch: 036, loss: 0.539 \n",
      "epoch: 001, batch: 037, loss: 0.570 \n",
      "epoch: 001, batch: 038, loss: 0.635 \n",
      "epoch: 001, batch: 039, loss: 0.885 \n",
      "epoch: 001, batch: 040, loss: 0.612 \n",
      "epoch: 001, batch: 041, loss: 0.769 \n",
      "epoch: 001, batch: 042, loss: 0.474 \n",
      "epoch: 001, batch: 043, loss: 0.716 \n",
      "epoch: 001, batch: 044, loss: 0.307 \n",
      "epoch: 001, batch: 045, loss: 0.983 \n",
      "epoch: 001, batch: 046, loss: 0.796 \n",
      "epoch: 001, batch: 047, loss: 0.315 \n",
      "epoch: 001, batch: 048, loss: 0.497 \n",
      "epoch: 001, batch: 049, loss: 0.811 \n",
      "epoch: 001, batch: 050, loss: 0.740 \n",
      "epoch: 001, batch: 051, loss: 0.551 \n",
      "epoch: 001, batch: 052, loss: 0.337 \n",
      "epoch: 001, batch: 053, loss: 0.221 \n",
      "epoch: 001, batch: 054, loss: 0.396 \n",
      "epoch: 001, batch: 055, loss: 1.048 \n",
      "epoch: 001, batch: 056, loss: 1.271 \n",
      "epoch: 001, batch: 057, loss: 0.825 \n",
      "epoch: 001, batch: 058, loss: 0.429 \n",
      "epoch: 001, batch: 059, loss: 0.353 \n",
      "epoch: 001, batch: 060, loss: 0.413 \n",
      "epoch: 001, batch: 061, loss: 2.389 \n",
      "epoch: 001, batch: 062, loss: 0.534 \n",
      "epoch: 001, batch: 063, loss: 0.276 \n",
      "epoch: 001, batch: 064, loss: 0.549 \n",
      "epoch: 001, batch: 065, loss: 0.630 \n",
      "epoch: 001, batch: 066, loss: 0.521 \n",
      "epoch: 001, batch: 067, loss: 0.360 \n",
      "epoch: 001, batch: 068, loss: 0.382 \n",
      "epoch: 001, batch: 069, loss: 0.298 \n",
      "epoch: 001, batch: 070, loss: 0.374 \n",
      "epoch: 001, batch: 071, loss: 0.381 \n",
      "epoch: 001, batch: 072, loss: 0.265 \n",
      "epoch: 001, batch: 073, loss: 0.704 \n",
      "epoch: 001, batch: 074, loss: 0.389 \n",
      "epoch: 001, batch: 075, loss: 1.225 \n",
      "epoch: 001, batch: 076, loss: 1.313 \n",
      "epoch: 001, batch: 077, loss: 0.779 \n",
      "epoch: 001, batch: 078, loss: 0.668 \n",
      "epoch: 001, batch: 079, loss: 0.439 \n",
      "epoch: 001, batch: 080, loss: 0.332 \n",
      "epoch: 001, batch: 081, loss: 0.344 \n",
      "epoch: 001, batch: 082, loss: 1.176 \n",
      "epoch: 001, batch: 083, loss: 0.900 \n",
      "epoch: 001, batch: 084, loss: 0.433 \n",
      "epoch: 001, batch: 085, loss: 1.350 \n",
      "epoch: 001, batch: 086, loss: 0.498 \n",
      "epoch: 001, batch: 087, loss: 0.438 \n",
      "epoch: 001, batch: 088, loss: 1.804 \n",
      "epoch: 001, batch: 089, loss: 0.622 \n",
      "epoch: 001, batch: 090, loss: 0.770 \n",
      "epoch: 001, batch: 091, loss: 0.450 \n",
      "epoch: 001, batch: 092, loss: 0.406 \n",
      "epoch: 001, batch: 093, loss: 0.419 \n",
      "epoch: 001, batch: 094, loss: 0.500 \n",
      "epoch: 001, batch: 095, loss: 0.507 \n",
      "epoch: 001, batch: 096, loss: 0.128 \n",
      "epoch: 001, batch: 097, loss: 0.398 \n",
      "epoch: 001, batch: 098, loss: 0.051 \n",
      "epoch: 001, batch: 099, loss: 0.807 \n",
      "epoch: 001, batch: 100, loss: 0.277 \n",
      "epoch: 001, batch: 101, loss: 0.983 \n",
      "epoch: 001, batch: 102, loss: 0.507 \n",
      "epoch: 001, batch: 103, loss: 0.978 \n",
      "epoch: 001, batch: 104, loss: 0.117 \n",
      "epoch: 001, batch: 105, loss: 0.631 \n",
      "epoch: 001, batch: 106, loss: 0.616 \n",
      "epoch: 001, batch: 107, loss: 0.512 \n",
      "epoch: 001, batch: 108, loss: 0.350 \n",
      "epoch: 001, batch: 109, loss: 0.364 \n",
      "epoch: 001, batch: 110, loss: 0.429 \n",
      "epoch: 001, batch: 111, loss: 0.373 \n",
      "epoch: 001, batch: 112, loss: 0.572 \n",
      "epoch: 001, batch: 113, loss: 0.147 \n",
      "epoch: 001, batch: 114, loss: 0.531 \n",
      "epoch: 001, batch: 115, loss: 0.107 \n",
      "epoch: 001, batch: 116, loss: 0.117 \n",
      "epoch: 001, batch: 117, loss: 0.127 \n",
      "epoch: 001, batch: 118, loss: 0.348 \n",
      "epoch: 001, batch: 119, loss: 0.258 \n",
      "epoch: 001, batch: 120, loss: 0.387 \n",
      "epoch: 001, batch: 121, loss: 0.486 \n",
      "epoch: 001, batch: 122, loss: 0.259 \n",
      "epoch: 001, batch: 123, loss: 0.324 \n",
      "epoch: 001, batch: 124, loss: 0.446 \n",
      "epoch: 001, batch: 125, loss: 0.683 \n",
      "epoch: 001, batch: 126, loss: 0.521 \n",
      "epoch: 001 ------------------------------------------------\n",
      "\n",
      "[train] loss: 72.343\n",
      "\n",
      "\n",
      "[train] accuracy: 91.681%\n",
      "\n",
      "[train] bulls_recall: 35.388%\n",
      "\n",
      "[train] bulls_precision: 25.949%\n",
      "\n",
      "[train] no_bulls_recall: 94.659%\n",
      "\n",
      "[train] no_bulls_precision: 96.515%\n",
      "\n",
      "[validation] accuracy: 40.466%\n",
      "\n",
      "[validation] bulls_recall: 66.413%\n",
      "\n",
      "[validation] bulls_precision: 3.143%\n",
      "\n",
      "[validation] no_bulls_recall: 39.701%\n",
      "\n",
      "[validation] no_bulls_precision: 97.568%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5305695008862314\n",
      "\n",
      "epoch: 002, batch: 001, loss: 0.614 \n",
      "epoch: 002, batch: 002, loss: 0.458 \n",
      "epoch: 002, batch: 003, loss: 0.180 \n",
      "epoch: 002, batch: 004, loss: 0.199 \n",
      "epoch: 002, batch: 005, loss: 0.351 \n",
      "epoch: 002, batch: 006, loss: 0.949 \n",
      "epoch: 002, batch: 007, loss: 0.380 \n",
      "epoch: 002, batch: 008, loss: 0.513 \n",
      "epoch: 002, batch: 009, loss: 1.350 \n",
      "epoch: 002, batch: 010, loss: 0.324 \n",
      "epoch: 002, batch: 011, loss: 0.216 \n",
      "epoch: 002, batch: 012, loss: 0.318 \n",
      "epoch: 002, batch: 013, loss: 0.198 \n",
      "epoch: 002, batch: 014, loss: 0.561 \n",
      "epoch: 002, batch: 015, loss: 0.180 \n",
      "epoch: 002, batch: 016, loss: 1.298 \n",
      "epoch: 002, batch: 017, loss: 1.500 \n",
      "epoch: 002, batch: 018, loss: 0.647 \n",
      "epoch: 002, batch: 019, loss: 0.336 \n",
      "epoch: 002, batch: 020, loss: 0.336 \n",
      "epoch: 002, batch: 021, loss: 0.713 \n",
      "epoch: 002, batch: 022, loss: 0.411 \n",
      "epoch: 002, batch: 023, loss: 0.600 \n",
      "epoch: 002, batch: 024, loss: 2.441 \n",
      "epoch: 002, batch: 025, loss: 1.001 \n",
      "epoch: 002, batch: 026, loss: 0.370 \n",
      "epoch: 002, batch: 027, loss: 0.761 \n",
      "epoch: 002, batch: 028, loss: 0.250 \n",
      "epoch: 002, batch: 029, loss: 0.417 \n",
      "epoch: 002, batch: 030, loss: 0.312 \n",
      "epoch: 002, batch: 031, loss: 0.722 \n",
      "epoch: 002, batch: 032, loss: 0.832 \n",
      "epoch: 002, batch: 033, loss: 0.258 \n",
      "epoch: 002, batch: 034, loss: 0.895 \n",
      "epoch: 002, batch: 035, loss: 0.084 \n",
      "epoch: 002, batch: 036, loss: 1.005 \n",
      "epoch: 002, batch: 037, loss: 0.429 \n",
      "epoch: 002, batch: 038, loss: 0.775 \n",
      "epoch: 002, batch: 039, loss: 0.493 \n",
      "epoch: 002, batch: 040, loss: 0.086 \n",
      "epoch: 002, batch: 041, loss: 0.265 \n",
      "epoch: 002, batch: 042, loss: 0.595 \n",
      "epoch: 002, batch: 043, loss: 0.172 \n",
      "epoch: 002, batch: 044, loss: 0.328 \n",
      "epoch: 002, batch: 045, loss: 0.315 \n",
      "epoch: 002, batch: 046, loss: 0.252 \n",
      "epoch: 002, batch: 047, loss: 1.851 \n",
      "epoch: 002, batch: 048, loss: 0.390 \n",
      "epoch: 002, batch: 049, loss: 0.374 \n",
      "epoch: 002, batch: 050, loss: 0.324 \n",
      "epoch: 002, batch: 051, loss: 0.886 \n",
      "epoch: 002, batch: 052, loss: 0.825 \n",
      "epoch: 002, batch: 053, loss: 0.591 \n",
      "epoch: 002, batch: 054, loss: 0.320 \n",
      "epoch: 002, batch: 055, loss: 0.353 \n",
      "epoch: 002, batch: 056, loss: 0.622 \n",
      "epoch: 002, batch: 057, loss: 0.478 \n",
      "epoch: 002, batch: 058, loss: 0.340 \n",
      "epoch: 002, batch: 059, loss: 0.318 \n",
      "epoch: 002, batch: 060, loss: 0.674 \n",
      "epoch: 002, batch: 061, loss: 0.245 \n",
      "epoch: 002, batch: 062, loss: 0.300 \n",
      "epoch: 002, batch: 063, loss: 0.480 \n",
      "epoch: 002, batch: 064, loss: 0.664 \n",
      "epoch: 002, batch: 065, loss: 0.359 \n",
      "epoch: 002, batch: 066, loss: 0.596 \n",
      "epoch: 002, batch: 067, loss: 0.407 \n",
      "epoch: 002, batch: 068, loss: 0.383 \n",
      "epoch: 002, batch: 069, loss: 0.245 \n",
      "epoch: 002, batch: 070, loss: 0.816 \n",
      "epoch: 002, batch: 071, loss: 0.198 \n",
      "epoch: 002, batch: 072, loss: 0.157 \n",
      "epoch: 002, batch: 073, loss: 0.130 \n",
      "epoch: 002, batch: 074, loss: 0.207 \n",
      "epoch: 002, batch: 075, loss: 0.102 \n",
      "epoch: 002, batch: 076, loss: 0.186 \n",
      "epoch: 002, batch: 077, loss: 0.411 \n",
      "epoch: 002, batch: 078, loss: 0.158 \n",
      "epoch: 002, batch: 079, loss: 0.542 \n",
      "epoch: 002, batch: 080, loss: 0.582 \n",
      "epoch: 002, batch: 081, loss: 0.212 \n",
      "epoch: 002, batch: 082, loss: 0.989 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 002, batch: 083, loss: 0.375 \n",
      "epoch: 002, batch: 084, loss: 0.676 \n",
      "epoch: 002, batch: 085, loss: 0.150 \n",
      "epoch: 002, batch: 086, loss: 2.842 \n",
      "epoch: 002, batch: 087, loss: 0.347 \n",
      "epoch: 002, batch: 088, loss: 0.483 \n",
      "epoch: 002, batch: 089, loss: 0.346 \n",
      "epoch: 002, batch: 090, loss: 0.070 \n",
      "epoch: 002, batch: 091, loss: 0.312 \n",
      "epoch: 002, batch: 092, loss: 0.233 \n",
      "epoch: 002, batch: 093, loss: 0.277 \n",
      "epoch: 002, batch: 094, loss: 0.344 \n",
      "epoch: 002, batch: 095, loss: 0.712 \n",
      "epoch: 002, batch: 096, loss: 0.744 \n",
      "epoch: 002, batch: 097, loss: 0.995 \n",
      "epoch: 002, batch: 098, loss: 0.345 \n",
      "epoch: 002, batch: 099, loss: 0.894 \n",
      "epoch: 002, batch: 100, loss: 0.375 \n",
      "epoch: 002, batch: 101, loss: 0.478 \n",
      "epoch: 002, batch: 102, loss: 0.171 \n",
      "epoch: 002, batch: 103, loss: 0.394 \n",
      "epoch: 002, batch: 104, loss: 0.294 \n",
      "epoch: 002, batch: 105, loss: 0.205 \n",
      "epoch: 002, batch: 106, loss: 0.720 \n",
      "epoch: 002, batch: 107, loss: 0.716 \n",
      "epoch: 002, batch: 108, loss: 0.242 \n",
      "epoch: 002, batch: 109, loss: 0.276 \n",
      "epoch: 002, batch: 110, loss: 0.422 \n",
      "epoch: 002, batch: 111, loss: 0.172 \n",
      "epoch: 002, batch: 112, loss: 0.177 \n",
      "epoch: 002, batch: 113, loss: 0.912 \n",
      "epoch: 002, batch: 114, loss: 0.263 \n",
      "epoch: 002, batch: 115, loss: 0.165 \n",
      "epoch: 002, batch: 116, loss: 0.471 \n",
      "epoch: 002, batch: 117, loss: 0.284 \n",
      "epoch: 002, batch: 118, loss: 0.538 \n",
      "epoch: 002, batch: 119, loss: 0.272 \n",
      "epoch: 002, batch: 120, loss: 0.325 \n",
      "epoch: 002, batch: 121, loss: 0.534 \n",
      "epoch: 002, batch: 122, loss: 0.408 \n",
      "epoch: 002, batch: 123, loss: 0.223 \n",
      "epoch: 002, batch: 124, loss: 0.510 \n",
      "epoch: 002, batch: 125, loss: 0.188 \n",
      "epoch: 002, batch: 126, loss: 0.800 \n",
      "epoch: 002 ------------------------------------------------\n",
      "\n",
      "[train] loss: 60.999\n",
      "\n",
      "\n",
      "[train] accuracy: 92.796%\n",
      "\n",
      "[train] bulls_recall: 39.950%\n",
      "\n",
      "[train] bulls_precision: 32.400%\n",
      "\n",
      "[train] no_bulls_recall: 95.591%\n",
      "\n",
      "[train] no_bulls_precision: 96.784%\n",
      "\n",
      "[validation] accuracy: 84.326%\n",
      "\n",
      "[validation] bulls_recall: 82.830%\n",
      "\n",
      "[validation] bulls_precision: 13.503%\n",
      "\n",
      "[validation] no_bulls_recall: 84.370%\n",
      "\n",
      "[validation] no_bulls_precision: 99.404%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8359982268030681\n",
      "\n",
      "epoch: 003, batch: 001, loss: 0.325 \n",
      "epoch: 003, batch: 002, loss: 0.330 \n",
      "epoch: 003, batch: 003, loss: 0.169 \n",
      "epoch: 003, batch: 004, loss: 0.865 \n",
      "epoch: 003, batch: 005, loss: 0.381 \n",
      "epoch: 003, batch: 006, loss: 0.199 \n",
      "epoch: 003, batch: 007, loss: 0.227 \n",
      "epoch: 003, batch: 008, loss: 0.176 \n",
      "epoch: 003, batch: 009, loss: 0.859 \n",
      "epoch: 003, batch: 010, loss: 0.800 \n",
      "epoch: 003, batch: 011, loss: 0.163 \n",
      "epoch: 003, batch: 012, loss: 0.215 \n",
      "epoch: 003, batch: 013, loss: 0.205 \n",
      "epoch: 003, batch: 014, loss: 0.368 \n",
      "epoch: 003, batch: 015, loss: 0.899 \n",
      "epoch: 003, batch: 016, loss: 0.323 \n",
      "epoch: 003, batch: 017, loss: 0.566 \n",
      "epoch: 003, batch: 018, loss: 0.233 \n",
      "epoch: 003, batch: 019, loss: 0.709 \n",
      "epoch: 003, batch: 020, loss: 0.759 \n",
      "epoch: 003, batch: 021, loss: 0.386 \n",
      "epoch: 003, batch: 022, loss: 0.319 \n",
      "epoch: 003, batch: 023, loss: 0.467 \n",
      "epoch: 003, batch: 024, loss: 0.342 \n",
      "epoch: 003, batch: 025, loss: 0.153 \n",
      "epoch: 003, batch: 026, loss: 0.091 \n",
      "epoch: 003, batch: 027, loss: 0.297 \n",
      "epoch: 003, batch: 028, loss: 0.480 \n",
      "epoch: 003, batch: 029, loss: 0.418 \n",
      "epoch: 003, batch: 030, loss: 0.461 \n",
      "epoch: 003, batch: 031, loss: 0.178 \n",
      "epoch: 003, batch: 032, loss: 0.353 \n",
      "epoch: 003, batch: 033, loss: 0.630 \n",
      "epoch: 003, batch: 034, loss: 0.329 \n",
      "epoch: 003, batch: 035, loss: 0.386 \n",
      "epoch: 003, batch: 036, loss: 0.304 \n",
      "epoch: 003, batch: 037, loss: 0.507 \n",
      "epoch: 003, batch: 038, loss: 0.235 \n",
      "epoch: 003, batch: 039, loss: 0.554 \n",
      "epoch: 003, batch: 040, loss: 0.243 \n",
      "epoch: 003, batch: 041, loss: 1.085 \n",
      "epoch: 003, batch: 042, loss: 0.231 \n",
      "epoch: 003, batch: 043, loss: 0.706 \n",
      "epoch: 003, batch: 044, loss: 0.245 \n",
      "epoch: 003, batch: 045, loss: 0.583 \n",
      "epoch: 003, batch: 046, loss: 0.310 \n",
      "epoch: 003, batch: 047, loss: 0.451 \n",
      "epoch: 003, batch: 048, loss: 0.066 \n",
      "epoch: 003, batch: 049, loss: 0.399 \n",
      "epoch: 003, batch: 050, loss: 0.658 \n",
      "epoch: 003, batch: 051, loss: 0.211 \n",
      "epoch: 003, batch: 052, loss: 0.241 \n",
      "epoch: 003, batch: 053, loss: 0.228 \n",
      "epoch: 003, batch: 054, loss: 0.230 \n",
      "epoch: 003, batch: 055, loss: 0.168 \n",
      "epoch: 003, batch: 056, loss: 0.325 \n",
      "epoch: 003, batch: 057, loss: 0.430 \n",
      "epoch: 003, batch: 058, loss: 0.414 \n",
      "epoch: 003, batch: 059, loss: 2.336 \n",
      "epoch: 003, batch: 060, loss: 0.266 \n",
      "epoch: 003, batch: 061, loss: 0.317 \n",
      "epoch: 003, batch: 062, loss: 0.402 \n",
      "epoch: 003, batch: 063, loss: 0.260 \n",
      "epoch: 003, batch: 064, loss: 0.029 \n",
      "epoch: 003, batch: 065, loss: 0.874 \n",
      "epoch: 003, batch: 066, loss: 0.153 \n",
      "epoch: 003, batch: 067, loss: 0.189 \n",
      "epoch: 003, batch: 068, loss: 0.285 \n",
      "epoch: 003, batch: 069, loss: 0.285 \n",
      "epoch: 003, batch: 070, loss: 0.252 \n",
      "epoch: 003, batch: 071, loss: 0.174 \n",
      "epoch: 003, batch: 072, loss: 0.130 \n",
      "epoch: 003, batch: 073, loss: 0.137 \n",
      "epoch: 003, batch: 074, loss: 0.358 \n",
      "epoch: 003, batch: 075, loss: 0.143 \n",
      "epoch: 003, batch: 076, loss: 0.166 \n",
      "epoch: 003, batch: 077, loss: 0.239 \n",
      "epoch: 003, batch: 078, loss: 0.151 \n",
      "epoch: 003, batch: 079, loss: 1.181 \n",
      "epoch: 003, batch: 080, loss: 0.070 \n",
      "epoch: 003, batch: 081, loss: 0.089 \n",
      "epoch: 003, batch: 082, loss: 0.033 \n",
      "epoch: 003, batch: 083, loss: 1.589 \n",
      "epoch: 003, batch: 084, loss: 1.238 \n",
      "epoch: 003, batch: 085, loss: 0.363 \n",
      "epoch: 003, batch: 086, loss: 0.097 \n",
      "epoch: 003, batch: 087, loss: 0.833 \n",
      "epoch: 003, batch: 088, loss: 0.593 \n",
      "epoch: 003, batch: 089, loss: 0.336 \n",
      "epoch: 003, batch: 090, loss: 0.280 \n",
      "epoch: 003, batch: 091, loss: 0.361 \n",
      "epoch: 003, batch: 092, loss: 0.219 \n",
      "epoch: 003, batch: 093, loss: 0.327 \n",
      "epoch: 003, batch: 094, loss: 1.671 \n",
      "epoch: 003, batch: 095, loss: 0.380 \n",
      "epoch: 003, batch: 096, loss: 0.249 \n",
      "epoch: 003, batch: 097, loss: 0.113 \n",
      "epoch: 003, batch: 098, loss: 0.205 \n",
      "epoch: 003, batch: 099, loss: 0.825 \n",
      "epoch: 003, batch: 100, loss: 0.906 \n",
      "epoch: 003, batch: 101, loss: 0.097 \n",
      "epoch: 003, batch: 102, loss: 0.548 \n",
      "epoch: 003, batch: 103, loss: 0.342 \n",
      "epoch: 003, batch: 104, loss: 0.293 \n",
      "epoch: 003, batch: 105, loss: 0.155 \n",
      "epoch: 003, batch: 106, loss: 0.695 \n",
      "epoch: 003, batch: 107, loss: 0.682 \n",
      "epoch: 003, batch: 108, loss: 0.779 \n",
      "epoch: 003, batch: 109, loss: 0.640 \n",
      "epoch: 003, batch: 110, loss: 0.583 \n",
      "epoch: 003, batch: 111, loss: 0.366 \n",
      "epoch: 003, batch: 112, loss: 0.498 \n",
      "epoch: 003, batch: 113, loss: 0.664 \n",
      "epoch: 003, batch: 114, loss: 2.662 \n",
      "epoch: 003, batch: 115, loss: 0.349 \n",
      "epoch: 003, batch: 116, loss: 0.354 \n",
      "epoch: 003, batch: 117, loss: 0.327 \n",
      "epoch: 003, batch: 118, loss: 0.301 \n",
      "epoch: 003, batch: 119, loss: 0.268 \n",
      "epoch: 003, batch: 120, loss: 0.188 \n",
      "epoch: 003, batch: 121, loss: 0.293 \n",
      "epoch: 003, batch: 122, loss: 0.127 \n",
      "epoch: 003, batch: 123, loss: 0.204 \n",
      "epoch: 003, batch: 124, loss: 0.237 \n",
      "epoch: 003, batch: 125, loss: 0.667 \n",
      "epoch: 003, batch: 126, loss: 0.182 \n",
      "epoch: 003 ------------------------------------------------\n",
      "\n",
      "[train] loss: 56.955\n",
      "\n",
      "\n",
      "[train] accuracy: 92.152%\n",
      "\n",
      "[train] bulls_recall: 47.595%\n",
      "\n",
      "[train] bulls_precision: 31.433%\n",
      "\n",
      "[train] no_bulls_recall: 94.509%\n",
      "\n",
      "[train] no_bulls_precision: 97.151%\n",
      "\n",
      "[validation] accuracy: 80.312%\n",
      "\n",
      "[validation] bulls_recall: 93.277%\n",
      "\n",
      "[validation] bulls_precision: 12.042%\n",
      "\n",
      "[validation] no_bulls_recall: 79.929%\n",
      "\n",
      "[validation] no_bulls_precision: 99.753%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8660305015453775\n",
      "\n",
      "epoch: 004, batch: 001, loss: 0.194 \n",
      "epoch: 004, batch: 002, loss: 0.870 \n",
      "epoch: 004, batch: 003, loss: 0.223 \n",
      "epoch: 004, batch: 004, loss: 3.537 \n",
      "epoch: 004, batch: 005, loss: 0.852 \n",
      "epoch: 004, batch: 006, loss: 0.366 \n",
      "epoch: 004, batch: 007, loss: 0.332 \n",
      "epoch: 004, batch: 008, loss: 0.153 \n",
      "epoch: 004, batch: 009, loss: 0.544 \n",
      "epoch: 004, batch: 010, loss: 0.633 \n",
      "epoch: 004, batch: 011, loss: 0.093 \n",
      "epoch: 004, batch: 012, loss: 0.104 \n",
      "epoch: 004, batch: 013, loss: 0.104 \n",
      "epoch: 004, batch: 014, loss: 0.636 \n",
      "epoch: 004, batch: 015, loss: 0.233 \n",
      "epoch: 004, batch: 016, loss: 0.603 \n",
      "epoch: 004, batch: 017, loss: 0.248 \n",
      "epoch: 004, batch: 018, loss: 0.369 \n",
      "epoch: 004, batch: 019, loss: 0.284 \n",
      "epoch: 004, batch: 020, loss: 0.730 \n",
      "epoch: 004, batch: 021, loss: 0.481 \n",
      "epoch: 004, batch: 022, loss: 0.206 \n",
      "epoch: 004, batch: 023, loss: 0.359 \n",
      "epoch: 004, batch: 024, loss: 0.241 \n",
      "epoch: 004, batch: 025, loss: 0.150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 004, batch: 026, loss: 0.238 \n",
      "epoch: 004, batch: 027, loss: 0.151 \n",
      "epoch: 004, batch: 028, loss: 0.176 \n",
      "epoch: 004, batch: 029, loss: 0.162 \n",
      "epoch: 004, batch: 030, loss: 0.822 \n",
      "epoch: 004, batch: 031, loss: 1.238 \n",
      "epoch: 004, batch: 032, loss: 0.152 \n",
      "epoch: 004, batch: 033, loss: 0.553 \n",
      "epoch: 004, batch: 034, loss: 0.662 \n",
      "epoch: 004, batch: 035, loss: 0.088 \n",
      "epoch: 004, batch: 036, loss: 0.419 \n",
      "epoch: 004, batch: 037, loss: 0.265 \n",
      "epoch: 004, batch: 038, loss: 0.088 \n",
      "epoch: 004, batch: 039, loss: 0.198 \n",
      "epoch: 004, batch: 040, loss: 0.204 \n",
      "epoch: 004, batch: 041, loss: 0.893 \n",
      "epoch: 004, batch: 042, loss: 0.294 \n",
      "epoch: 004, batch: 043, loss: 0.830 \n",
      "epoch: 004, batch: 044, loss: 0.171 \n",
      "epoch: 004, batch: 045, loss: 0.846 \n",
      "epoch: 004, batch: 046, loss: 0.245 \n",
      "epoch: 004, batch: 047, loss: 0.453 \n",
      "epoch: 004, batch: 048, loss: 0.668 \n",
      "epoch: 004, batch: 049, loss: 0.195 \n",
      "epoch: 004, batch: 050, loss: 0.300 \n",
      "epoch: 004, batch: 051, loss: 0.490 \n",
      "epoch: 004, batch: 052, loss: 0.259 \n",
      "epoch: 004, batch: 053, loss: 0.140 \n",
      "epoch: 004, batch: 054, loss: 0.542 \n",
      "epoch: 004, batch: 055, loss: 0.211 \n",
      "epoch: 004, batch: 056, loss: 1.186 \n",
      "epoch: 004, batch: 057, loss: 0.565 \n",
      "epoch: 004, batch: 058, loss: 1.077 \n",
      "epoch: 004, batch: 059, loss: 1.364 \n",
      "epoch: 004, batch: 060, loss: 0.130 \n",
      "epoch: 004, batch: 061, loss: 0.912 \n",
      "epoch: 004, batch: 062, loss: 0.071 \n",
      "epoch: 004, batch: 063, loss: 0.084 \n",
      "epoch: 004, batch: 064, loss: 0.259 \n",
      "epoch: 004, batch: 065, loss: 0.095 \n",
      "epoch: 004, batch: 066, loss: 0.414 \n",
      "epoch: 004, batch: 067, loss: 0.280 \n",
      "epoch: 004, batch: 068, loss: 0.100 \n",
      "epoch: 004, batch: 069, loss: 0.864 \n",
      "epoch: 004, batch: 070, loss: 0.275 \n",
      "epoch: 004, batch: 071, loss: 0.151 \n",
      "epoch: 004, batch: 072, loss: 0.303 \n",
      "epoch: 004, batch: 073, loss: 0.638 \n",
      "epoch: 004, batch: 074, loss: 0.758 \n",
      "epoch: 004, batch: 075, loss: 0.717 \n",
      "epoch: 004, batch: 076, loss: 0.099 \n",
      "epoch: 004, batch: 077, loss: 0.179 \n",
      "epoch: 004, batch: 078, loss: 0.788 \n",
      "epoch: 004, batch: 079, loss: 0.037 \n",
      "epoch: 004, batch: 080, loss: 0.077 \n",
      "epoch: 004, batch: 081, loss: 0.297 \n",
      "epoch: 004, batch: 082, loss: 0.095 \n",
      "epoch: 004, batch: 083, loss: 0.228 \n",
      "epoch: 004, batch: 084, loss: 0.125 \n",
      "epoch: 004, batch: 085, loss: 0.734 \n",
      "epoch: 004, batch: 086, loss: 0.328 \n",
      "epoch: 004, batch: 087, loss: 0.301 \n",
      "epoch: 004, batch: 088, loss: 0.502 \n",
      "epoch: 004, batch: 089, loss: 0.483 \n",
      "epoch: 004, batch: 090, loss: 0.076 \n",
      "epoch: 004, batch: 091, loss: 0.300 \n",
      "epoch: 004, batch: 092, loss: 1.003 \n",
      "epoch: 004, batch: 093, loss: 0.333 \n",
      "epoch: 004, batch: 094, loss: 0.354 \n",
      "epoch: 004, batch: 095, loss: 0.223 \n",
      "epoch: 004, batch: 096, loss: 0.818 \n",
      "epoch: 004, batch: 097, loss: 0.259 \n",
      "epoch: 004, batch: 098, loss: 0.342 \n",
      "epoch: 004, batch: 099, loss: 0.388 \n",
      "epoch: 004, batch: 100, loss: 0.741 \n",
      "epoch: 004, batch: 101, loss: 0.615 \n",
      "epoch: 004, batch: 102, loss: 0.127 \n",
      "epoch: 004, batch: 103, loss: 0.627 \n",
      "epoch: 004, batch: 104, loss: 0.243 \n",
      "epoch: 004, batch: 105, loss: 0.378 \n",
      "epoch: 004, batch: 106, loss: 0.272 \n",
      "epoch: 004, batch: 107, loss: 0.147 \n",
      "epoch: 004, batch: 108, loss: 0.280 \n",
      "epoch: 004, batch: 109, loss: 0.232 \n",
      "epoch: 004, batch: 110, loss: 0.200 \n",
      "epoch: 004, batch: 111, loss: 0.090 \n",
      "epoch: 004, batch: 112, loss: 0.179 \n",
      "epoch: 004, batch: 113, loss: 0.102 \n",
      "epoch: 004, batch: 114, loss: 1.374 \n",
      "epoch: 004, batch: 115, loss: 0.628 \n",
      "epoch: 004, batch: 116, loss: 0.133 \n",
      "epoch: 004, batch: 117, loss: 0.208 \n",
      "epoch: 004, batch: 118, loss: 0.212 \n",
      "epoch: 004, batch: 119, loss: 0.239 \n",
      "epoch: 004, batch: 120, loss: 1.016 \n",
      "epoch: 004, batch: 121, loss: 0.445 \n",
      "epoch: 004, batch: 122, loss: 0.333 \n",
      "epoch: 004, batch: 123, loss: 0.273 \n",
      "epoch: 004, batch: 124, loss: 0.395 \n",
      "epoch: 004, batch: 125, loss: 0.594 \n",
      "epoch: 004, batch: 126, loss: 0.277 \n",
      "epoch: 004 ------------------------------------------------\n",
      "\n",
      "[train] loss: 51.513\n",
      "\n",
      "\n",
      "[train] accuracy: 88.757%\n",
      "\n",
      "[train] bulls_recall: 53.267%\n",
      "\n",
      "[train] bulls_precision: 23.126%\n",
      "\n",
      "[train] no_bulls_recall: 90.635%\n",
      "\n",
      "[train] no_bulls_precision: 97.345%\n",
      "\n",
      "[validation] accuracy: 75.934%\n",
      "\n",
      "[validation] bulls_recall: 98.500%\n",
      "\n",
      "[validation] bulls_precision: 10.501%\n",
      "\n",
      "[validation] no_bulls_recall: 75.269%\n",
      "\n",
      "[validation] no_bulls_precision: 99.941%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8688461821138433\n",
      "\n",
      "epoch: 005, batch: 001, loss: 0.197 \n",
      "epoch: 005, batch: 002, loss: 0.354 \n",
      "epoch: 005, batch: 003, loss: 0.244 \n",
      "epoch: 005, batch: 004, loss: 0.287 \n",
      "epoch: 005, batch: 005, loss: 0.416 \n",
      "epoch: 005, batch: 006, loss: 0.168 \n",
      "epoch: 005, batch: 007, loss: 0.105 \n",
      "epoch: 005, batch: 008, loss: 0.369 \n",
      "epoch: 005, batch: 009, loss: 0.321 \n",
      "epoch: 005, batch: 010, loss: 0.156 \n",
      "epoch: 005, batch: 011, loss: 0.429 \n",
      "epoch: 005, batch: 012, loss: 0.833 \n",
      "epoch: 005, batch: 013, loss: 0.173 \n",
      "epoch: 005, batch: 014, loss: 0.165 \n",
      "epoch: 005, batch: 015, loss: 0.263 \n",
      "epoch: 005, batch: 016, loss: 0.761 \n",
      "epoch: 005, batch: 017, loss: 0.250 \n",
      "epoch: 005, batch: 018, loss: 0.489 \n",
      "epoch: 005, batch: 019, loss: 0.677 \n",
      "epoch: 005, batch: 020, loss: 0.541 \n",
      "epoch: 005, batch: 021, loss: 0.220 \n",
      "epoch: 005, batch: 022, loss: 0.254 \n",
      "epoch: 005, batch: 023, loss: 0.337 \n",
      "epoch: 005, batch: 024, loss: 0.818 \n",
      "epoch: 005, batch: 025, loss: 0.137 \n",
      "epoch: 005, batch: 026, loss: 0.619 \n",
      "epoch: 005, batch: 027, loss: 0.184 \n",
      "epoch: 005, batch: 028, loss: 0.175 \n",
      "epoch: 005, batch: 029, loss: 0.268 \n",
      "epoch: 005, batch: 030, loss: 0.176 \n",
      "epoch: 005, batch: 031, loss: 0.367 \n",
      "epoch: 005, batch: 032, loss: 0.654 \n",
      "epoch: 005, batch: 033, loss: 0.402 \n",
      "epoch: 005, batch: 034, loss: 1.065 \n",
      "epoch: 005, batch: 035, loss: 0.197 \n",
      "epoch: 005, batch: 036, loss: 0.253 \n",
      "epoch: 005, batch: 037, loss: 0.153 \n",
      "epoch: 005, batch: 038, loss: 0.169 \n",
      "epoch: 005, batch: 039, loss: 0.120 \n",
      "epoch: 005, batch: 040, loss: 0.669 \n",
      "epoch: 005, batch: 041, loss: 0.169 \n",
      "epoch: 005, batch: 042, loss: 0.506 \n",
      "epoch: 005, batch: 043, loss: 0.182 \n",
      "epoch: 005, batch: 044, loss: 0.129 \n",
      "epoch: 005, batch: 045, loss: 0.116 \n",
      "epoch: 005, batch: 046, loss: 0.631 \n",
      "epoch: 005, batch: 047, loss: 0.225 \n",
      "epoch: 005, batch: 048, loss: 0.632 \n",
      "epoch: 005, batch: 049, loss: 0.706 \n",
      "epoch: 005, batch: 050, loss: 0.165 \n",
      "epoch: 005, batch: 051, loss: 1.392 \n",
      "epoch: 005, batch: 052, loss: 0.902 \n",
      "epoch: 005, batch: 053, loss: 0.278 \n",
      "epoch: 005, batch: 054, loss: 0.071 \n",
      "epoch: 005, batch: 055, loss: 0.171 \n",
      "epoch: 005, batch: 056, loss: 0.475 \n",
      "epoch: 005, batch: 057, loss: 1.371 \n",
      "epoch: 005, batch: 058, loss: 0.081 \n",
      "epoch: 005, batch: 059, loss: 0.421 \n",
      "epoch: 005, batch: 060, loss: 0.185 \n",
      "epoch: 005, batch: 061, loss: 0.120 \n",
      "epoch: 005, batch: 062, loss: 0.153 \n",
      "epoch: 005, batch: 063, loss: 0.480 \n",
      "epoch: 005, batch: 064, loss: 0.110 \n",
      "epoch: 005, batch: 065, loss: 0.829 \n",
      "epoch: 005, batch: 066, loss: 0.463 \n",
      "epoch: 005, batch: 067, loss: 0.489 \n",
      "epoch: 005, batch: 068, loss: 0.244 \n",
      "epoch: 005, batch: 069, loss: 0.308 \n",
      "epoch: 005, batch: 070, loss: 1.054 \n",
      "epoch: 005, batch: 071, loss: 0.095 \n",
      "epoch: 005, batch: 072, loss: 0.173 \n",
      "epoch: 005, batch: 073, loss: 0.103 \n",
      "epoch: 005, batch: 074, loss: 0.530 \n",
      "epoch: 005, batch: 075, loss: 0.389 \n",
      "epoch: 005, batch: 076, loss: 0.862 \n",
      "epoch: 005, batch: 077, loss: 0.269 \n",
      "epoch: 005, batch: 078, loss: 0.335 \n",
      "epoch: 005, batch: 079, loss: 0.330 \n",
      "epoch: 005, batch: 080, loss: 0.979 \n",
      "epoch: 005, batch: 081, loss: 0.285 \n",
      "epoch: 005, batch: 082, loss: 0.262 \n",
      "epoch: 005, batch: 083, loss: 0.222 \n",
      "epoch: 005, batch: 084, loss: 0.504 \n",
      "epoch: 005, batch: 085, loss: 0.095 \n",
      "epoch: 005, batch: 086, loss: 0.176 \n",
      "epoch: 005, batch: 087, loss: 0.373 \n",
      "epoch: 005, batch: 088, loss: 0.285 \n",
      "epoch: 005, batch: 089, loss: 0.286 \n",
      "epoch: 005, batch: 090, loss: 0.161 \n",
      "epoch: 005, batch: 091, loss: 0.088 \n",
      "epoch: 005, batch: 092, loss: 0.107 \n",
      "epoch: 005, batch: 093, loss: 0.251 \n",
      "epoch: 005, batch: 094, loss: 0.828 \n",
      "epoch: 005, batch: 095, loss: 0.267 \n",
      "epoch: 005, batch: 096, loss: 0.254 \n",
      "epoch: 005, batch: 097, loss: 0.125 \n",
      "epoch: 005, batch: 098, loss: 0.224 \n",
      "epoch: 005, batch: 099, loss: 0.195 \n",
      "epoch: 005, batch: 100, loss: 0.179 \n",
      "epoch: 005, batch: 101, loss: 0.161 \n",
      "epoch: 005, batch: 102, loss: 0.378 \n",
      "epoch: 005, batch: 103, loss: 0.949 \n",
      "epoch: 005, batch: 104, loss: 0.104 \n",
      "epoch: 005, batch: 105, loss: 0.152 \n",
      "epoch: 005, batch: 106, loss: 0.145 \n",
      "epoch: 005, batch: 107, loss: 0.162 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 005, batch: 108, loss: 0.118 \n",
      "epoch: 005, batch: 109, loss: 0.428 \n",
      "epoch: 005, batch: 110, loss: 0.205 \n",
      "epoch: 005, batch: 111, loss: 0.399 \n",
      "epoch: 005, batch: 112, loss: 0.255 \n",
      "epoch: 005, batch: 113, loss: 0.110 \n",
      "epoch: 005, batch: 114, loss: 0.266 \n",
      "epoch: 005, batch: 115, loss: 0.478 \n",
      "epoch: 005, batch: 116, loss: 0.295 \n",
      "epoch: 005, batch: 117, loss: 0.550 \n",
      "epoch: 005, batch: 118, loss: 0.389 \n",
      "epoch: 005, batch: 119, loss: 0.488 \n",
      "epoch: 005, batch: 120, loss: 0.847 \n",
      "epoch: 005, batch: 121, loss: 0.140 \n",
      "epoch: 005, batch: 122, loss: 0.057 \n",
      "epoch: 005, batch: 123, loss: 0.176 \n",
      "epoch: 005, batch: 124, loss: 0.224 \n",
      "epoch: 005, batch: 125, loss: 0.569 \n",
      "epoch: 005, batch: 126, loss: 0.009 \n",
      "epoch: 005 ------------------------------------------------\n",
      "\n",
      "[train] loss: 49.067\n",
      "\n",
      "\n",
      "[train] accuracy: 94.184%\n",
      "\n",
      "[train] bulls_recall: 44.266%\n",
      "\n",
      "[train] bulls_precision: 42.434%\n",
      "\n",
      "[train] no_bulls_recall: 96.824%\n",
      "\n",
      "[train] no_bulls_precision: 97.045%\n",
      "\n",
      "[validation] accuracy: 72.646%\n",
      "\n",
      "[validation] bulls_recall: 76.114%\n",
      "\n",
      "[validation] bulls_precision: 7.550%\n",
      "\n",
      "[validation] no_bulls_recall: 72.543%\n",
      "\n",
      "[validation] no_bulls_precision: 99.039%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.7432848976057008\n",
      "\n",
      "epoch: 006, batch: 001, loss: 0.695 \n",
      "epoch: 006, batch: 002, loss: 0.273 \n",
      "epoch: 006, batch: 003, loss: 0.083 \n",
      "epoch: 006, batch: 004, loss: 0.161 \n",
      "epoch: 006, batch: 005, loss: 0.106 \n",
      "epoch: 006, batch: 006, loss: 1.114 \n",
      "epoch: 006, batch: 007, loss: 0.552 \n",
      "epoch: 006, batch: 008, loss: 0.215 \n",
      "epoch: 006, batch: 009, loss: 0.129 \n",
      "epoch: 006, batch: 010, loss: 0.761 \n",
      "epoch: 006, batch: 011, loss: 0.209 \n",
      "epoch: 006, batch: 012, loss: 0.294 \n",
      "epoch: 006, batch: 013, loss: 0.135 \n",
      "epoch: 006, batch: 014, loss: 0.056 \n",
      "epoch: 006, batch: 015, loss: 0.191 \n",
      "epoch: 006, batch: 016, loss: 0.133 \n",
      "epoch: 006, batch: 017, loss: 0.187 \n",
      "epoch: 006, batch: 018, loss: 0.507 \n",
      "epoch: 006, batch: 019, loss: 0.671 \n",
      "epoch: 006, batch: 020, loss: 0.095 \n",
      "epoch: 006, batch: 021, loss: 0.244 \n",
      "epoch: 006, batch: 022, loss: 0.413 \n",
      "epoch: 006, batch: 023, loss: 0.392 \n",
      "epoch: 006, batch: 024, loss: 0.663 \n",
      "epoch: 006, batch: 025, loss: 1.459 \n",
      "epoch: 006, batch: 026, loss: 0.717 \n",
      "epoch: 006, batch: 027, loss: 0.328 \n",
      "epoch: 006, batch: 028, loss: 0.189 \n",
      "epoch: 006, batch: 029, loss: 0.370 \n",
      "epoch: 006, batch: 030, loss: 0.907 \n",
      "epoch: 006, batch: 031, loss: 0.166 \n",
      "epoch: 006, batch: 032, loss: 0.853 \n",
      "epoch: 006, batch: 033, loss: 0.074 \n",
      "epoch: 006, batch: 034, loss: 1.428 \n",
      "epoch: 006, batch: 035, loss: 1.796 \n",
      "epoch: 006, batch: 036, loss: 0.649 \n",
      "epoch: 006, batch: 037, loss: 0.134 \n",
      "epoch: 006, batch: 038, loss: 0.290 \n",
      "epoch: 006, batch: 039, loss: 0.391 \n",
      "epoch: 006, batch: 040, loss: 0.806 \n",
      "epoch: 006, batch: 041, loss: 0.333 \n",
      "epoch: 006, batch: 042, loss: 0.177 \n",
      "epoch: 006, batch: 043, loss: 0.056 \n",
      "epoch: 006, batch: 044, loss: 0.418 \n",
      "epoch: 006, batch: 045, loss: 0.320 \n",
      "epoch: 006, batch: 046, loss: 0.709 \n",
      "epoch: 006, batch: 047, loss: 1.148 \n",
      "epoch: 006, batch: 048, loss: 1.557 \n",
      "epoch: 006, batch: 049, loss: 0.187 \n",
      "epoch: 006, batch: 050, loss: 0.085 \n",
      "epoch: 006, batch: 051, loss: 0.161 \n",
      "epoch: 006, batch: 052, loss: 0.094 \n",
      "epoch: 006, batch: 053, loss: 0.067 \n",
      "epoch: 006, batch: 054, loss: 0.971 \n",
      "epoch: 006, batch: 055, loss: 0.019 \n",
      "epoch: 006, batch: 056, loss: 0.144 \n",
      "epoch: 006, batch: 057, loss: 0.554 \n",
      "epoch: 006, batch: 058, loss: 0.411 \n",
      "epoch: 006, batch: 059, loss: 0.087 \n",
      "epoch: 006, batch: 060, loss: 0.410 \n",
      "epoch: 006, batch: 061, loss: 0.629 \n",
      "epoch: 006, batch: 062, loss: 0.212 \n",
      "epoch: 006, batch: 063, loss: 0.536 \n",
      "epoch: 006, batch: 064, loss: 0.334 \n",
      "epoch: 006, batch: 065, loss: 0.813 \n",
      "epoch: 006, batch: 066, loss: 0.238 \n",
      "epoch: 006, batch: 067, loss: 0.129 \n",
      "epoch: 006, batch: 068, loss: 0.327 \n",
      "epoch: 006, batch: 069, loss: 0.237 \n",
      "epoch: 006, batch: 070, loss: 0.089 \n",
      "epoch: 006, batch: 071, loss: 0.140 \n",
      "epoch: 006, batch: 072, loss: 0.289 \n",
      "epoch: 006, batch: 073, loss: 1.373 \n",
      "epoch: 006, batch: 074, loss: 0.525 \n",
      "epoch: 006, batch: 075, loss: 0.800 \n",
      "epoch: 006, batch: 076, loss: 0.321 \n",
      "epoch: 006, batch: 077, loss: 0.887 \n",
      "epoch: 006, batch: 078, loss: 0.251 \n",
      "epoch: 006, batch: 079, loss: 0.166 \n",
      "epoch: 006, batch: 080, loss: 0.226 \n",
      "epoch: 006, batch: 081, loss: 0.265 \n",
      "epoch: 006, batch: 082, loss: 0.063 \n",
      "epoch: 006, batch: 083, loss: 1.404 \n",
      "epoch: 006, batch: 084, loss: 0.106 \n",
      "epoch: 006, batch: 085, loss: 0.165 \n",
      "epoch: 006, batch: 086, loss: 0.531 \n",
      "epoch: 006, batch: 087, loss: 0.045 \n",
      "epoch: 006, batch: 088, loss: 0.567 \n",
      "epoch: 006, batch: 089, loss: 0.747 \n",
      "epoch: 006, batch: 090, loss: 0.211 \n",
      "epoch: 006, batch: 091, loss: 0.379 \n",
      "epoch: 006, batch: 092, loss: 0.223 \n",
      "epoch: 006, batch: 093, loss: 0.260 \n",
      "epoch: 006, batch: 094, loss: 0.135 \n",
      "epoch: 006, batch: 095, loss: 0.821 \n",
      "epoch: 006, batch: 096, loss: 0.491 \n",
      "epoch: 006, batch: 097, loss: 0.821 \n",
      "epoch: 006, batch: 098, loss: 0.074 \n",
      "epoch: 006, batch: 099, loss: 0.386 \n",
      "epoch: 006, batch: 100, loss: 0.915 \n",
      "epoch: 006, batch: 101, loss: 0.618 \n",
      "epoch: 006, batch: 102, loss: 0.152 \n",
      "epoch: 006, batch: 103, loss: 0.337 \n",
      "epoch: 006, batch: 104, loss: 0.315 \n",
      "epoch: 006, batch: 105, loss: 0.127 \n",
      "epoch: 006, batch: 106, loss: 0.597 \n",
      "epoch: 006, batch: 107, loss: 1.218 \n",
      "epoch: 006, batch: 108, loss: 0.312 \n",
      "epoch: 006, batch: 109, loss: 0.631 \n",
      "epoch: 006, batch: 110, loss: 0.413 \n",
      "epoch: 006, batch: 111, loss: 0.364 \n",
      "epoch: 006, batch: 112, loss: 0.213 \n",
      "epoch: 006, batch: 113, loss: 0.525 \n",
      "epoch: 006, batch: 114, loss: 0.454 \n",
      "epoch: 006, batch: 115, loss: 0.117 \n",
      "epoch: 006, batch: 116, loss: 0.381 \n",
      "epoch: 006, batch: 117, loss: 0.360 \n",
      "epoch: 006, batch: 118, loss: 0.141 \n",
      "epoch: 006, batch: 119, loss: 0.214 \n",
      "epoch: 006, batch: 120, loss: 0.592 \n",
      "epoch: 006, batch: 121, loss: 0.083 \n",
      "epoch: 006, batch: 122, loss: 0.110 \n",
      "epoch: 006, batch: 123, loss: 0.390 \n",
      "epoch: 006, batch: 124, loss: 0.155 \n",
      "epoch: 006, batch: 125, loss: 0.381 \n",
      "epoch: 006, batch: 126, loss: 0.038 \n",
      "epoch: 006 ------------------------------------------------\n",
      "\n",
      "[train] loss: 46.203\n",
      "\n",
      "\n",
      "[train] accuracy: 95.076%\n",
      "\n",
      "[train] bulls_recall: 44.759%\n",
      "\n",
      "[train] bulls_precision: 51.126%\n",
      "\n",
      "[train] no_bulls_recall: 97.737%\n",
      "\n",
      "[train] no_bulls_precision: 97.097%\n",
      "\n",
      "[validation] accuracy: 81.486%\n",
      "\n",
      "[validation] bulls_recall: 79.845%\n",
      "\n",
      "[validation] bulls_precision: 11.299%\n",
      "\n",
      "[validation] no_bulls_recall: 81.534%\n",
      "\n",
      "[validation] no_bulls_precision: 99.277%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8068950581622609\n",
      "\n",
      "epoch: 007, batch: 001, loss: 0.814 \n",
      "epoch: 007, batch: 002, loss: 1.007 \n",
      "epoch: 007, batch: 003, loss: 0.113 \n",
      "epoch: 007, batch: 004, loss: 0.184 \n",
      "epoch: 007, batch: 005, loss: 0.489 \n",
      "epoch: 007, batch: 006, loss: 0.229 \n",
      "epoch: 007, batch: 007, loss: 0.134 \n",
      "epoch: 007, batch: 008, loss: 0.607 \n",
      "epoch: 007, batch: 009, loss: 0.199 \n",
      "epoch: 007, batch: 010, loss: 0.100 \n",
      "epoch: 007, batch: 011, loss: 0.351 \n",
      "epoch: 007, batch: 012, loss: 0.989 \n",
      "epoch: 007, batch: 013, loss: 0.160 \n",
      "epoch: 007, batch: 014, loss: 0.046 \n",
      "epoch: 007, batch: 015, loss: 0.150 \n",
      "epoch: 007, batch: 016, loss: 0.120 \n",
      "epoch: 007, batch: 017, loss: 0.066 \n",
      "epoch: 007, batch: 018, loss: 0.097 \n",
      "epoch: 007, batch: 019, loss: 0.237 \n",
      "epoch: 007, batch: 020, loss: 0.074 \n",
      "epoch: 007, batch: 021, loss: 0.223 \n",
      "epoch: 007, batch: 022, loss: 0.562 \n",
      "epoch: 007, batch: 023, loss: 0.476 \n",
      "epoch: 007, batch: 024, loss: 0.466 \n",
      "epoch: 007, batch: 025, loss: 0.397 \n",
      "epoch: 007, batch: 026, loss: 0.575 \n",
      "epoch: 007, batch: 027, loss: 0.281 \n",
      "epoch: 007, batch: 028, loss: 0.246 \n",
      "epoch: 007, batch: 029, loss: 0.720 \n",
      "epoch: 007, batch: 030, loss: 0.122 \n",
      "epoch: 007, batch: 031, loss: 0.177 \n",
      "epoch: 007, batch: 032, loss: 0.328 \n",
      "epoch: 007, batch: 033, loss: 0.105 \n",
      "epoch: 007, batch: 034, loss: 0.065 \n",
      "epoch: 007, batch: 035, loss: 1.928 \n",
      "epoch: 007, batch: 036, loss: 1.197 \n",
      "epoch: 007, batch: 037, loss: 0.116 \n",
      "epoch: 007, batch: 038, loss: 0.128 \n",
      "epoch: 007, batch: 039, loss: 0.366 \n",
      "epoch: 007, batch: 040, loss: 0.157 \n",
      "epoch: 007, batch: 041, loss: 0.189 \n",
      "epoch: 007, batch: 042, loss: 0.523 \n",
      "epoch: 007, batch: 043, loss: 0.280 \n",
      "epoch: 007, batch: 044, loss: 0.039 \n",
      "epoch: 007, batch: 045, loss: 0.694 \n",
      "epoch: 007, batch: 046, loss: 0.355 \n",
      "epoch: 007, batch: 047, loss: 0.420 \n",
      "epoch: 007, batch: 048, loss: 0.715 \n",
      "epoch: 007, batch: 049, loss: 0.329 \n",
      "epoch: 007, batch: 050, loss: 0.356 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 007, batch: 051, loss: 0.903 \n",
      "epoch: 007, batch: 052, loss: 0.146 \n",
      "epoch: 007, batch: 053, loss: 0.815 \n",
      "epoch: 007, batch: 054, loss: 0.291 \n",
      "epoch: 007, batch: 055, loss: 0.279 \n",
      "epoch: 007, batch: 056, loss: 0.082 \n",
      "epoch: 007, batch: 057, loss: 0.043 \n",
      "epoch: 007, batch: 058, loss: 0.075 \n",
      "epoch: 007, batch: 059, loss: 0.686 \n",
      "epoch: 007, batch: 060, loss: 0.102 \n",
      "epoch: 007, batch: 061, loss: 0.323 \n",
      "epoch: 007, batch: 062, loss: 0.868 \n",
      "epoch: 007, batch: 063, loss: 0.715 \n",
      "epoch: 007, batch: 064, loss: 0.268 \n",
      "epoch: 007, batch: 065, loss: 0.995 \n",
      "epoch: 007, batch: 066, loss: 0.361 \n",
      "epoch: 007, batch: 067, loss: 0.127 \n",
      "epoch: 007, batch: 068, loss: 0.320 \n",
      "epoch: 007, batch: 069, loss: 0.281 \n",
      "epoch: 007, batch: 070, loss: 0.213 \n",
      "epoch: 007, batch: 071, loss: 0.287 \n",
      "epoch: 007, batch: 072, loss: 0.680 \n",
      "epoch: 007, batch: 073, loss: 0.949 \n",
      "epoch: 007, batch: 074, loss: 0.051 \n",
      "epoch: 007, batch: 075, loss: 0.202 \n",
      "epoch: 007, batch: 076, loss: 3.004 \n",
      "epoch: 007, batch: 077, loss: 0.300 \n",
      "epoch: 007, batch: 078, loss: 0.321 \n",
      "epoch: 007, batch: 079, loss: 0.122 \n",
      "epoch: 007, batch: 080, loss: 0.317 \n",
      "epoch: 007, batch: 081, loss: 0.131 \n",
      "epoch: 007, batch: 082, loss: 0.113 \n",
      "epoch: 007, batch: 083, loss: 0.239 \n",
      "epoch: 007, batch: 084, loss: 0.513 \n",
      "epoch: 007, batch: 085, loss: 0.615 \n",
      "epoch: 007, batch: 086, loss: 0.203 \n",
      "epoch: 007, batch: 087, loss: 0.209 \n",
      "epoch: 007, batch: 088, loss: 0.123 \n",
      "epoch: 007, batch: 089, loss: 0.861 \n",
      "epoch: 007, batch: 090, loss: 0.741 \n",
      "epoch: 007, batch: 091, loss: 0.192 \n",
      "epoch: 007, batch: 092, loss: 0.281 \n",
      "epoch: 007, batch: 093, loss: 0.061 \n",
      "epoch: 007, batch: 094, loss: 0.394 \n",
      "epoch: 007, batch: 095, loss: 0.249 \n",
      "epoch: 007, batch: 096, loss: 0.347 \n",
      "epoch: 007, batch: 097, loss: 0.104 \n",
      "epoch: 007, batch: 098, loss: 0.160 \n",
      "epoch: 007, batch: 099, loss: 0.090 \n",
      "epoch: 007, batch: 100, loss: 0.223 \n",
      "epoch: 007, batch: 101, loss: 0.168 \n",
      "epoch: 007, batch: 102, loss: 0.489 \n",
      "epoch: 007, batch: 103, loss: 0.072 \n",
      "epoch: 007, batch: 104, loss: 0.591 \n",
      "epoch: 007, batch: 105, loss: 0.208 \n",
      "epoch: 007, batch: 106, loss: 0.484 \n",
      "epoch: 007, batch: 107, loss: 0.140 \n",
      "epoch: 007, batch: 108, loss: 0.043 \n",
      "epoch: 007, batch: 109, loss: 0.042 \n",
      "epoch: 007, batch: 110, loss: 0.928 \n",
      "epoch: 007, batch: 111, loss: 0.198 \n",
      "epoch: 007, batch: 112, loss: 0.124 \n",
      "epoch: 007, batch: 113, loss: 0.136 \n",
      "epoch: 007, batch: 114, loss: 0.285 \n",
      "epoch: 007, batch: 115, loss: 0.110 \n",
      "epoch: 007, batch: 116, loss: 0.960 \n",
      "epoch: 007, batch: 117, loss: 0.169 \n",
      "epoch: 007, batch: 118, loss: 0.330 \n",
      "epoch: 007, batch: 119, loss: 1.961 \n",
      "epoch: 007, batch: 120, loss: 0.199 \n",
      "epoch: 007, batch: 121, loss: 0.422 \n",
      "epoch: 007, batch: 122, loss: 0.337 \n",
      "epoch: 007, batch: 123, loss: 0.254 \n",
      "epoch: 007, batch: 124, loss: 1.237 \n",
      "epoch: 007, batch: 125, loss: 0.947 \n",
      "epoch: 007, batch: 126, loss: 0.618 \n",
      "epoch: 007 ------------------------------------------------\n",
      "\n",
      "[train] loss: 46.510\n",
      "\n",
      "\n",
      "[train] accuracy: 92.654%\n",
      "\n",
      "[train] bulls_recall: 53.390%\n",
      "\n",
      "[train] bulls_precision: 34.891%\n",
      "\n",
      "[train] no_bulls_recall: 94.730%\n",
      "\n",
      "[train] no_bulls_precision: 97.464%\n",
      "\n",
      "[validation] accuracy: 52.210%\n",
      "\n",
      "[validation] bulls_recall: 97.754%\n",
      "\n",
      "[validation] bulls_precision: 5.537%\n",
      "\n",
      "[validation] no_bulls_recall: 50.868%\n",
      "\n",
      "[validation] no_bulls_precision: 99.870%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.7431105492410468\n",
      "\n",
      "epoch: 008, batch: 001, loss: 0.201 \n",
      "epoch: 008, batch: 002, loss: 0.211 \n",
      "epoch: 008, batch: 003, loss: 0.316 \n",
      "epoch: 008, batch: 004, loss: 0.149 \n",
      "epoch: 008, batch: 005, loss: 0.206 \n",
      "epoch: 008, batch: 006, loss: 0.450 \n",
      "epoch: 008, batch: 007, loss: 0.361 \n",
      "epoch: 008, batch: 008, loss: 0.044 \n",
      "epoch: 008, batch: 009, loss: 0.148 \n",
      "epoch: 008, batch: 010, loss: 0.037 \n",
      "epoch: 008, batch: 011, loss: 0.052 \n",
      "epoch: 008, batch: 012, loss: 0.122 \n",
      "epoch: 008, batch: 013, loss: 0.190 \n",
      "epoch: 008, batch: 014, loss: 0.085 \n",
      "epoch: 008, batch: 015, loss: 0.074 \n",
      "epoch: 008, batch: 016, loss: 0.582 \n",
      "epoch: 008, batch: 017, loss: 0.252 \n",
      "epoch: 008, batch: 018, loss: 0.820 \n",
      "epoch: 008, batch: 019, loss: 0.791 \n",
      "epoch: 008, batch: 020, loss: 0.594 \n",
      "epoch: 008, batch: 021, loss: 0.468 \n",
      "epoch: 008, batch: 022, loss: 0.551 \n",
      "epoch: 008, batch: 023, loss: 0.183 \n",
      "epoch: 008, batch: 024, loss: 0.239 \n",
      "epoch: 008, batch: 025, loss: 0.410 \n",
      "epoch: 008, batch: 026, loss: 0.008 \n",
      "epoch: 008, batch: 027, loss: 0.581 \n",
      "epoch: 008, batch: 028, loss: 0.450 \n",
      "epoch: 008, batch: 029, loss: 0.154 \n",
      "epoch: 008, batch: 030, loss: 0.721 \n",
      "epoch: 008, batch: 031, loss: 0.310 \n",
      "epoch: 008, batch: 032, loss: 0.062 \n",
      "epoch: 008, batch: 033, loss: 0.718 \n",
      "epoch: 008, batch: 034, loss: 0.396 \n",
      "epoch: 008, batch: 035, loss: 0.964 \n",
      "epoch: 008, batch: 036, loss: 0.360 \n",
      "epoch: 008, batch: 037, loss: 0.244 \n",
      "epoch: 008, batch: 038, loss: 0.226 \n",
      "epoch: 008, batch: 039, loss: 0.164 \n",
      "epoch: 008, batch: 040, loss: 0.421 \n",
      "epoch: 008, batch: 041, loss: 0.734 \n",
      "epoch: 008, batch: 042, loss: 0.182 \n",
      "epoch: 008, batch: 043, loss: 0.159 \n",
      "epoch: 008, batch: 044, loss: 0.978 \n",
      "epoch: 008, batch: 045, loss: 0.005 \n",
      "epoch: 008, batch: 046, loss: 0.049 \n",
      "epoch: 008, batch: 047, loss: 0.193 \n",
      "epoch: 008, batch: 048, loss: 0.210 \n",
      "epoch: 008, batch: 049, loss: 0.088 \n",
      "epoch: 008, batch: 050, loss: 0.194 \n",
      "epoch: 008, batch: 051, loss: 0.220 \n",
      "epoch: 008, batch: 052, loss: 0.067 \n",
      "epoch: 008, batch: 053, loss: 0.691 \n",
      "epoch: 008, batch: 054, loss: 1.150 \n",
      "epoch: 008, batch: 055, loss: 0.323 \n",
      "epoch: 008, batch: 056, loss: 0.182 \n",
      "epoch: 008, batch: 057, loss: 0.117 \n",
      "epoch: 008, batch: 058, loss: 0.060 \n",
      "epoch: 008, batch: 059, loss: 0.100 \n",
      "epoch: 008, batch: 060, loss: 0.611 \n",
      "epoch: 008, batch: 061, loss: 0.137 \n",
      "epoch: 008, batch: 062, loss: 0.115 \n",
      "epoch: 008, batch: 063, loss: 0.101 \n",
      "epoch: 008, batch: 064, loss: 0.329 \n",
      "epoch: 008, batch: 065, loss: 0.239 \n",
      "epoch: 008, batch: 066, loss: 0.178 \n",
      "epoch: 008, batch: 067, loss: 0.153 \n",
      "epoch: 008, batch: 068, loss: 0.088 \n",
      "epoch: 008, batch: 069, loss: 0.357 \n",
      "epoch: 008, batch: 070, loss: 0.067 \n",
      "epoch: 008, batch: 071, loss: 0.845 \n",
      "epoch: 008, batch: 072, loss: 0.893 \n",
      "epoch: 008, batch: 073, loss: 0.184 \n",
      "epoch: 008, batch: 074, loss: 0.153 \n",
      "epoch: 008, batch: 075, loss: 0.244 \n",
      "epoch: 008, batch: 076, loss: 0.290 \n",
      "epoch: 008, batch: 077, loss: 0.542 \n",
      "epoch: 008, batch: 078, loss: 0.338 \n",
      "epoch: 008, batch: 079, loss: 0.096 \n",
      "epoch: 008, batch: 080, loss: 0.245 \n",
      "epoch: 008, batch: 081, loss: 0.187 \n",
      "epoch: 008, batch: 082, loss: 0.412 \n",
      "epoch: 008, batch: 083, loss: 0.553 \n",
      "epoch: 008, batch: 084, loss: 0.231 \n",
      "epoch: 008, batch: 085, loss: 0.418 \n",
      "epoch: 008, batch: 086, loss: 0.236 \n",
      "epoch: 008, batch: 087, loss: 0.378 \n",
      "epoch: 008, batch: 088, loss: 0.165 \n",
      "epoch: 008, batch: 089, loss: 0.212 \n",
      "epoch: 008, batch: 090, loss: 0.165 \n",
      "epoch: 008, batch: 091, loss: 0.072 \n",
      "epoch: 008, batch: 092, loss: 0.183 \n",
      "epoch: 008, batch: 093, loss: 0.236 \n",
      "epoch: 008, batch: 094, loss: 0.226 \n",
      "epoch: 008, batch: 095, loss: 0.216 \n",
      "epoch: 008, batch: 096, loss: 0.963 \n",
      "epoch: 008, batch: 097, loss: 0.258 \n",
      "epoch: 008, batch: 098, loss: 0.776 \n",
      "epoch: 008, batch: 099, loss: 0.356 \n",
      "epoch: 008, batch: 100, loss: 0.082 \n",
      "epoch: 008, batch: 101, loss: 0.446 \n",
      "epoch: 008, batch: 102, loss: 0.166 \n",
      "epoch: 008, batch: 103, loss: 0.174 \n",
      "epoch: 008, batch: 104, loss: 0.542 \n",
      "epoch: 008, batch: 105, loss: 0.134 \n",
      "epoch: 008, batch: 106, loss: 0.106 \n",
      "epoch: 008, batch: 107, loss: 0.127 \n",
      "epoch: 008, batch: 108, loss: 0.405 \n",
      "epoch: 008, batch: 109, loss: 0.232 \n",
      "epoch: 008, batch: 110, loss: 0.205 \n",
      "epoch: 008, batch: 111, loss: 0.163 \n",
      "epoch: 008, batch: 112, loss: 0.893 \n",
      "epoch: 008, batch: 113, loss: 0.160 \n",
      "epoch: 008, batch: 114, loss: 0.725 \n",
      "epoch: 008, batch: 115, loss: 0.185 \n",
      "epoch: 008, batch: 116, loss: 0.114 \n",
      "epoch: 008, batch: 117, loss: 0.385 \n",
      "epoch: 008, batch: 118, loss: 0.295 \n",
      "epoch: 008, batch: 119, loss: 0.166 \n",
      "epoch: 008, batch: 120, loss: 0.200 \n",
      "epoch: 008, batch: 121, loss: 0.380 \n",
      "epoch: 008, batch: 122, loss: 0.127 \n",
      "epoch: 008, batch: 123, loss: 0.203 \n",
      "epoch: 008, batch: 124, loss: 0.119 \n",
      "epoch: 008, batch: 125, loss: 0.433 \n",
      "epoch: 008, batch: 126, loss: 0.484 \n",
      "epoch: 008 ------------------------------------------------\n",
      "\n",
      "[train] loss: 43.432\n",
      "\n",
      "\n",
      "[train] accuracy: 93.892%\n",
      "\n",
      "[train] bulls_recall: 51.541%\n",
      "\n",
      "[train] bulls_precision: 41.345%\n",
      "\n",
      "[train] no_bulls_recall: 96.132%\n",
      "\n",
      "[train] no_bulls_precision: 97.403%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] accuracy: 80.952%\n",
      "\n",
      "[validation] bulls_recall: 85.068%\n",
      "\n",
      "[validation] bulls_precision: 11.562%\n",
      "\n",
      "[validation] no_bulls_recall: 80.831%\n",
      "\n",
      "[validation] no_bulls_precision: 99.459%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8294952632756277\n",
      "\n",
      "epoch: 009, batch: 001, loss: 1.358 \n",
      "epoch: 009, batch: 002, loss: 0.482 \n",
      "epoch: 009, batch: 003, loss: 0.118 \n",
      "epoch: 009, batch: 004, loss: 0.184 \n",
      "epoch: 009, batch: 005, loss: 0.187 \n",
      "epoch: 009, batch: 006, loss: 0.258 \n",
      "epoch: 009, batch: 007, loss: 0.112 \n",
      "epoch: 009, batch: 008, loss: 0.221 \n",
      "epoch: 009, batch: 009, loss: 0.110 \n",
      "epoch: 009, batch: 010, loss: 0.186 \n",
      "epoch: 009, batch: 011, loss: 0.185 \n",
      "epoch: 009, batch: 012, loss: 0.294 \n",
      "epoch: 009, batch: 013, loss: 0.182 \n",
      "epoch: 009, batch: 014, loss: 0.043 \n",
      "epoch: 009, batch: 015, loss: 0.910 \n",
      "epoch: 009, batch: 016, loss: 0.834 \n",
      "epoch: 009, batch: 017, loss: 0.339 \n",
      "epoch: 009, batch: 018, loss: 0.096 \n",
      "epoch: 009, batch: 019, loss: 0.174 \n",
      "epoch: 009, batch: 020, loss: 0.241 \n",
      "epoch: 009, batch: 021, loss: 0.757 \n",
      "epoch: 009, batch: 022, loss: 1.058 \n",
      "epoch: 009, batch: 023, loss: 0.309 \n",
      "epoch: 009, batch: 024, loss: 0.101 \n",
      "epoch: 009, batch: 025, loss: 0.553 \n",
      "epoch: 009, batch: 026, loss: 0.085 \n",
      "epoch: 009, batch: 027, loss: 0.849 \n",
      "epoch: 009, batch: 028, loss: 0.050 \n",
      "epoch: 009, batch: 029, loss: 0.787 \n",
      "epoch: 009, batch: 030, loss: 0.094 \n",
      "epoch: 009, batch: 031, loss: 0.062 \n",
      "epoch: 009, batch: 032, loss: 0.311 \n",
      "epoch: 009, batch: 033, loss: 0.182 \n",
      "epoch: 009, batch: 034, loss: 0.375 \n",
      "epoch: 009, batch: 035, loss: 0.233 \n",
      "epoch: 009, batch: 036, loss: 0.482 \n",
      "epoch: 009, batch: 037, loss: 0.592 \n",
      "epoch: 009, batch: 038, loss: 1.420 \n",
      "epoch: 009, batch: 039, loss: 0.424 \n",
      "epoch: 009, batch: 040, loss: 0.456 \n",
      "epoch: 009, batch: 041, loss: 0.798 \n",
      "epoch: 009, batch: 042, loss: 2.325 \n",
      "epoch: 009, batch: 043, loss: 0.710 \n",
      "epoch: 009, batch: 044, loss: 0.116 \n",
      "epoch: 009, batch: 045, loss: 0.351 \n",
      "epoch: 009, batch: 046, loss: 0.146 \n",
      "epoch: 009, batch: 047, loss: 0.344 \n",
      "epoch: 009, batch: 048, loss: 0.139 \n",
      "epoch: 009, batch: 049, loss: 0.364 \n",
      "epoch: 009, batch: 050, loss: 0.365 \n",
      "epoch: 009, batch: 051, loss: 0.077 \n",
      "epoch: 009, batch: 052, loss: 0.441 \n",
      "epoch: 009, batch: 053, loss: 0.200 \n",
      "epoch: 009, batch: 054, loss: 0.313 \n",
      "epoch: 009, batch: 055, loss: 0.067 \n",
      "epoch: 009, batch: 056, loss: 0.134 \n",
      "epoch: 009, batch: 057, loss: 0.224 \n",
      "epoch: 009, batch: 058, loss: 0.110 \n",
      "epoch: 009, batch: 059, loss: 0.552 \n",
      "epoch: 009, batch: 060, loss: 0.377 \n",
      "epoch: 009, batch: 061, loss: 0.329 \n",
      "epoch: 009, batch: 062, loss: 0.363 \n",
      "epoch: 009, batch: 063, loss: 0.109 \n",
      "epoch: 009, batch: 064, loss: 1.409 \n",
      "epoch: 009, batch: 065, loss: 0.201 \n",
      "epoch: 009, batch: 066, loss: 0.502 \n",
      "epoch: 009, batch: 067, loss: 0.068 \n",
      "epoch: 009, batch: 068, loss: 0.025 \n",
      "epoch: 009, batch: 069, loss: 0.135 \n",
      "epoch: 009, batch: 070, loss: 0.728 \n",
      "epoch: 009, batch: 071, loss: 0.206 \n",
      "epoch: 009, batch: 072, loss: 0.105 \n",
      "epoch: 009, batch: 073, loss: 0.315 \n",
      "epoch: 009, batch: 074, loss: 0.661 \n",
      "epoch: 009, batch: 075, loss: 0.109 \n",
      "epoch: 009, batch: 076, loss: 0.016 \n",
      "epoch: 009, batch: 077, loss: 0.594 \n",
      "epoch: 009, batch: 078, loss: 0.142 \n",
      "epoch: 009, batch: 079, loss: 0.091 \n",
      "epoch: 009, batch: 080, loss: 0.095 \n",
      "epoch: 009, batch: 081, loss: 0.826 \n",
      "epoch: 009, batch: 082, loss: 0.354 \n",
      "epoch: 009, batch: 083, loss: 0.153 \n",
      "epoch: 009, batch: 084, loss: 0.015 \n",
      "epoch: 009, batch: 085, loss: 0.404 \n",
      "epoch: 009, batch: 086, loss: 0.173 \n",
      "epoch: 009, batch: 087, loss: 1.022 \n",
      "epoch: 009, batch: 088, loss: 0.174 \n",
      "epoch: 009, batch: 089, loss: 0.196 \n",
      "epoch: 009, batch: 090, loss: 0.706 \n",
      "epoch: 009, batch: 091, loss: 0.068 \n",
      "epoch: 009, batch: 092, loss: 0.280 \n",
      "epoch: 009, batch: 093, loss: 0.309 \n",
      "epoch: 009, batch: 094, loss: 0.018 \n",
      "epoch: 009, batch: 095, loss: 0.330 \n",
      "epoch: 009, batch: 096, loss: 0.118 \n",
      "epoch: 009, batch: 097, loss: 0.300 \n",
      "epoch: 009, batch: 098, loss: 0.281 \n",
      "epoch: 009, batch: 099, loss: 0.056 \n",
      "epoch: 009, batch: 100, loss: 0.242 \n",
      "epoch: 009, batch: 101, loss: 0.049 \n",
      "epoch: 009, batch: 102, loss: 0.230 \n",
      "epoch: 009, batch: 103, loss: 0.042 \n",
      "epoch: 009, batch: 104, loss: 1.833 \n",
      "epoch: 009, batch: 105, loss: 0.099 \n",
      "epoch: 009, batch: 106, loss: 0.873 \n",
      "epoch: 009, batch: 107, loss: 0.174 \n",
      "epoch: 009, batch: 108, loss: 0.348 \n",
      "epoch: 009, batch: 109, loss: 0.223 \n",
      "epoch: 009, batch: 110, loss: 0.423 \n",
      "epoch: 009, batch: 111, loss: 0.335 \n",
      "epoch: 009, batch: 112, loss: 0.371 \n",
      "epoch: 009, batch: 113, loss: 0.131 \n",
      "epoch: 009, batch: 114, loss: 0.640 \n",
      "epoch: 009, batch: 115, loss: 0.783 \n",
      "epoch: 009, batch: 116, loss: 0.095 \n",
      "epoch: 009, batch: 117, loss: 0.100 \n",
      "epoch: 009, batch: 118, loss: 0.224 \n",
      "epoch: 009, batch: 119, loss: 0.174 \n",
      "epoch: 009, batch: 120, loss: 0.062 \n",
      "epoch: 009, batch: 121, loss: 0.024 \n",
      "epoch: 009, batch: 122, loss: 0.123 \n",
      "epoch: 009, batch: 123, loss: 0.072 \n",
      "epoch: 009, batch: 124, loss: 0.046 \n",
      "epoch: 009, batch: 125, loss: 0.105 \n",
      "epoch: 009, batch: 126, loss: 0.126 \n",
      "epoch: 009 ------------------------------------------------\n",
      "\n",
      "[train] loss: 40.966\n",
      "\n",
      "\n",
      "[train] accuracy: 94.450%\n",
      "\n",
      "[train] bulls_recall: 55.363%\n",
      "\n",
      "[train] bulls_precision: 45.676%\n",
      "\n",
      "[train] no_bulls_recall: 96.517%\n",
      "\n",
      "[train] no_bulls_precision: 97.612%\n",
      "\n",
      "[validation] accuracy: 80.589%\n",
      "\n",
      "[validation] bulls_recall: 84.322%\n",
      "\n",
      "[validation] bulls_precision: 11.289%\n",
      "\n",
      "[validation] no_bulls_recall: 80.479%\n",
      "\n",
      "[validation] no_bulls_precision: 99.429%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8240055740257295\n",
      "\n",
      "epoch: 010, batch: 001, loss: 0.272 \n",
      "epoch: 010, batch: 002, loss: 0.245 \n",
      "epoch: 010, batch: 003, loss: 0.023 \n",
      "epoch: 010, batch: 004, loss: 0.436 \n",
      "epoch: 010, batch: 005, loss: 0.102 \n",
      "epoch: 010, batch: 006, loss: 0.155 \n",
      "epoch: 010, batch: 007, loss: 0.059 \n",
      "epoch: 010, batch: 008, loss: 0.105 \n",
      "epoch: 010, batch: 009, loss: 0.324 \n",
      "epoch: 010, batch: 010, loss: 0.156 \n",
      "epoch: 010, batch: 011, loss: 0.375 \n",
      "epoch: 010, batch: 012, loss: 0.240 \n",
      "epoch: 010, batch: 013, loss: 0.447 \n",
      "epoch: 010, batch: 014, loss: 0.166 \n",
      "epoch: 010, batch: 015, loss: 0.512 \n",
      "epoch: 010, batch: 016, loss: 0.323 \n",
      "epoch: 010, batch: 017, loss: 0.224 \n",
      "epoch: 010, batch: 018, loss: 0.148 \n",
      "epoch: 010, batch: 019, loss: 0.046 \n",
      "epoch: 010, batch: 020, loss: 0.398 \n",
      "epoch: 010, batch: 021, loss: 0.964 \n",
      "epoch: 010, batch: 022, loss: 0.035 \n",
      "epoch: 010, batch: 023, loss: 0.232 \n",
      "epoch: 010, batch: 024, loss: 0.089 \n",
      "epoch: 010, batch: 025, loss: 0.059 \n",
      "epoch: 010, batch: 026, loss: 0.414 \n",
      "epoch: 010, batch: 027, loss: 0.135 \n",
      "epoch: 010, batch: 028, loss: 0.189 \n",
      "epoch: 010, batch: 029, loss: 0.120 \n",
      "epoch: 010, batch: 030, loss: 0.118 \n",
      "epoch: 010, batch: 031, loss: 0.524 \n",
      "epoch: 010, batch: 032, loss: 0.329 \n",
      "epoch: 010, batch: 033, loss: 0.447 \n",
      "epoch: 010, batch: 034, loss: 0.274 \n",
      "epoch: 010, batch: 035, loss: 0.470 \n",
      "epoch: 010, batch: 036, loss: 0.039 \n",
      "epoch: 010, batch: 037, loss: 0.215 \n",
      "epoch: 010, batch: 038, loss: 0.255 \n",
      "epoch: 010, batch: 039, loss: 0.967 \n",
      "epoch: 010, batch: 040, loss: 0.049 \n",
      "epoch: 010, batch: 041, loss: 0.417 \n",
      "epoch: 010, batch: 042, loss: 0.366 \n",
      "epoch: 010, batch: 043, loss: 0.214 \n",
      "epoch: 010, batch: 044, loss: 0.208 \n",
      "epoch: 010, batch: 045, loss: 2.320 \n",
      "epoch: 010, batch: 046, loss: 0.113 \n",
      "epoch: 010, batch: 047, loss: 0.334 \n",
      "epoch: 010, batch: 048, loss: 0.647 \n",
      "epoch: 010, batch: 049, loss: 0.245 \n",
      "epoch: 010, batch: 050, loss: 0.697 \n",
      "epoch: 010, batch: 051, loss: 0.414 \n",
      "epoch: 010, batch: 052, loss: 0.437 \n",
      "epoch: 010, batch: 053, loss: 0.301 \n",
      "epoch: 010, batch: 054, loss: 0.031 \n",
      "epoch: 010, batch: 055, loss: 0.519 \n",
      "epoch: 010, batch: 056, loss: 0.463 \n",
      "epoch: 010, batch: 057, loss: 0.086 \n",
      "epoch: 010, batch: 058, loss: 0.001 \n",
      "epoch: 010, batch: 059, loss: 0.151 \n",
      "epoch: 010, batch: 060, loss: 0.161 \n",
      "epoch: 010, batch: 061, loss: 0.852 \n",
      "epoch: 010, batch: 062, loss: 0.728 \n",
      "epoch: 010, batch: 063, loss: 0.680 \n",
      "epoch: 010, batch: 064, loss: 0.012 \n",
      "epoch: 010, batch: 065, loss: 0.258 \n",
      "epoch: 010, batch: 066, loss: 0.078 \n",
      "epoch: 010, batch: 067, loss: 0.206 \n",
      "epoch: 010, batch: 068, loss: 0.155 \n",
      "epoch: 010, batch: 069, loss: 0.412 \n",
      "epoch: 010, batch: 070, loss: 0.548 \n",
      "epoch: 010, batch: 071, loss: 0.127 \n",
      "epoch: 010, batch: 072, loss: 1.055 \n",
      "epoch: 010, batch: 073, loss: 0.154 \n",
      "epoch: 010, batch: 074, loss: 0.226 \n",
      "epoch: 010, batch: 075, loss: 0.158 \n",
      "epoch: 010, batch: 076, loss: 0.143 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 010, batch: 077, loss: 0.029 \n",
      "epoch: 010, batch: 078, loss: 0.259 \n",
      "epoch: 010, batch: 079, loss: 0.153 \n",
      "epoch: 010, batch: 080, loss: 0.236 \n",
      "epoch: 010, batch: 081, loss: 0.038 \n",
      "epoch: 010, batch: 082, loss: 1.222 \n",
      "epoch: 010, batch: 083, loss: 0.130 \n",
      "epoch: 010, batch: 084, loss: 0.224 \n",
      "epoch: 010, batch: 085, loss: 0.625 \n",
      "epoch: 010, batch: 086, loss: 0.318 \n",
      "epoch: 010, batch: 087, loss: 0.410 \n",
      "epoch: 010, batch: 088, loss: 0.318 \n",
      "epoch: 010, batch: 089, loss: 1.180 \n",
      "epoch: 010, batch: 090, loss: 0.257 \n",
      "epoch: 010, batch: 091, loss: 0.236 \n",
      "epoch: 010, batch: 092, loss: 0.173 \n",
      "epoch: 010, batch: 093, loss: 0.162 \n",
      "epoch: 010, batch: 094, loss: 1.250 \n",
      "epoch: 010, batch: 095, loss: 0.220 \n",
      "epoch: 010, batch: 096, loss: 0.717 \n",
      "epoch: 010, batch: 097, loss: 0.045 \n",
      "epoch: 010, batch: 098, loss: 1.203 \n",
      "epoch: 010, batch: 099, loss: 0.269 \n",
      "epoch: 010, batch: 100, loss: 0.191 \n",
      "epoch: 010, batch: 101, loss: 0.149 \n",
      "epoch: 010, batch: 102, loss: 0.078 \n",
      "epoch: 010, batch: 103, loss: 0.553 \n",
      "epoch: 010, batch: 104, loss: 0.313 \n",
      "epoch: 010, batch: 105, loss: 0.172 \n",
      "epoch: 010, batch: 106, loss: 0.180 \n",
      "epoch: 010, batch: 107, loss: 0.302 \n",
      "epoch: 010, batch: 108, loss: 0.094 \n",
      "epoch: 010, batch: 109, loss: 0.058 \n",
      "epoch: 010, batch: 110, loss: 0.028 \n",
      "epoch: 010, batch: 111, loss: 0.117 \n",
      "epoch: 010, batch: 112, loss: 0.399 \n",
      "epoch: 010, batch: 113, loss: 0.134 \n",
      "epoch: 010, batch: 114, loss: 0.041 \n",
      "epoch: 010, batch: 115, loss: 0.037 \n",
      "epoch: 010, batch: 116, loss: 0.177 \n",
      "epoch: 010, batch: 117, loss: 0.142 \n",
      "epoch: 010, batch: 118, loss: 0.021 \n",
      "epoch: 010, batch: 119, loss: 0.144 \n",
      "epoch: 010, batch: 120, loss: 0.790 \n",
      "epoch: 010, batch: 121, loss: 0.198 \n",
      "epoch: 010, batch: 122, loss: 0.053 \n",
      "epoch: 010, batch: 123, loss: 1.289 \n",
      "epoch: 010, batch: 124, loss: 0.883 \n",
      "epoch: 010, batch: 125, loss: 0.107 \n",
      "epoch: 010, batch: 126, loss: 0.103 \n",
      "epoch: 010 ------------------------------------------------\n",
      "\n",
      "[train] loss: 43.550\n",
      "\n",
      "\n",
      "[train] accuracy: 87.351%\n",
      "\n",
      "[train] bulls_recall: 71.146%\n",
      "\n",
      "[train] bulls_precision: 24.193%\n",
      "\n",
      "[train] no_bulls_recall: 88.208%\n",
      "\n",
      "[train] no_bulls_precision: 98.299%\n",
      "\n",
      "[validation] accuracy: 63.912%\n",
      "\n",
      "[validation] bulls_recall: 98.500%\n",
      "\n",
      "[validation] bulls_precision: 7.253%\n",
      "\n",
      "[validation] no_bulls_recall: 62.893%\n",
      "\n",
      "[validation] no_bulls_precision: 99.930%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.806964585898403\n",
      "\n",
      "epoch: 011, batch: 001, loss: 0.132 \n",
      "epoch: 011, batch: 002, loss: 0.524 \n",
      "epoch: 011, batch: 003, loss: 0.356 \n",
      "epoch: 011, batch: 004, loss: 0.303 \n",
      "epoch: 011, batch: 005, loss: 0.214 \n",
      "epoch: 011, batch: 006, loss: 0.128 \n",
      "epoch: 011, batch: 007, loss: 0.299 \n",
      "epoch: 011, batch: 008, loss: 0.438 \n",
      "epoch: 011, batch: 009, loss: 0.191 \n",
      "epoch: 011, batch: 010, loss: 0.073 \n",
      "epoch: 011, batch: 011, loss: 0.352 \n",
      "epoch: 011, batch: 012, loss: 0.126 \n",
      "epoch: 011, batch: 013, loss: 0.207 \n",
      "epoch: 011, batch: 014, loss: 0.468 \n",
      "epoch: 011, batch: 015, loss: 0.134 \n",
      "epoch: 011, batch: 016, loss: 0.974 \n",
      "epoch: 011, batch: 017, loss: 0.223 \n",
      "epoch: 011, batch: 018, loss: 0.291 \n",
      "epoch: 011, batch: 019, loss: 0.859 \n",
      "epoch: 011, batch: 020, loss: 0.259 \n",
      "epoch: 011, batch: 021, loss: 0.263 \n",
      "epoch: 011, batch: 022, loss: 0.107 \n",
      "epoch: 011, batch: 023, loss: 0.197 \n",
      "epoch: 011, batch: 024, loss: 0.080 \n",
      "epoch: 011, batch: 025, loss: 0.344 \n",
      "epoch: 011, batch: 026, loss: 0.201 \n",
      "epoch: 011, batch: 027, loss: 0.103 \n",
      "epoch: 011, batch: 028, loss: 0.790 \n",
      "epoch: 011, batch: 029, loss: 0.259 \n",
      "epoch: 011, batch: 030, loss: 0.437 \n",
      "epoch: 011, batch: 031, loss: 0.193 \n",
      "epoch: 011, batch: 032, loss: 1.585 \n",
      "epoch: 011, batch: 033, loss: 0.831 \n",
      "epoch: 011, batch: 034, loss: 0.269 \n",
      "epoch: 011, batch: 035, loss: 0.243 \n",
      "epoch: 011, batch: 036, loss: 0.110 \n",
      "epoch: 011, batch: 037, loss: 0.778 \n",
      "epoch: 011, batch: 038, loss: 0.235 \n",
      "epoch: 011, batch: 039, loss: 0.281 \n",
      "epoch: 011, batch: 040, loss: 0.295 \n",
      "epoch: 011, batch: 041, loss: 0.055 \n",
      "epoch: 011, batch: 042, loss: 0.047 \n",
      "epoch: 011, batch: 043, loss: 0.284 \n",
      "epoch: 011, batch: 044, loss: 0.662 \n",
      "epoch: 011, batch: 045, loss: 0.072 \n",
      "epoch: 011, batch: 046, loss: 1.022 \n",
      "epoch: 011, batch: 047, loss: 0.112 \n",
      "epoch: 011, batch: 048, loss: 0.071 \n",
      "epoch: 011, batch: 049, loss: 0.047 \n",
      "epoch: 011, batch: 050, loss: 0.148 \n",
      "epoch: 011, batch: 051, loss: 0.159 \n",
      "epoch: 011, batch: 052, loss: 0.189 \n",
      "epoch: 011, batch: 053, loss: 0.291 \n",
      "epoch: 011, batch: 054, loss: 0.708 \n",
      "epoch: 011, batch: 055, loss: 0.812 \n",
      "epoch: 011, batch: 056, loss: 0.230 \n",
      "epoch: 011, batch: 057, loss: 0.137 \n",
      "epoch: 011, batch: 058, loss: 0.214 \n",
      "epoch: 011, batch: 059, loss: 0.191 \n",
      "epoch: 011, batch: 060, loss: 0.271 \n",
      "epoch: 011, batch: 061, loss: 0.163 \n",
      "epoch: 011, batch: 062, loss: 0.229 \n",
      "epoch: 011, batch: 063, loss: 0.372 \n",
      "epoch: 011, batch: 064, loss: 0.211 \n",
      "epoch: 011, batch: 065, loss: 0.423 \n",
      "epoch: 011, batch: 066, loss: 0.779 \n",
      "epoch: 011, batch: 067, loss: 0.163 \n",
      "epoch: 011, batch: 068, loss: 0.159 \n",
      "epoch: 011, batch: 069, loss: 0.236 \n",
      "epoch: 011, batch: 070, loss: 0.074 \n",
      "epoch: 011, batch: 071, loss: 0.379 \n",
      "epoch: 011, batch: 072, loss: 0.280 \n",
      "epoch: 011, batch: 073, loss: 0.183 \n",
      "epoch: 011, batch: 074, loss: 0.283 \n",
      "epoch: 011, batch: 075, loss: 0.142 \n",
      "epoch: 011, batch: 076, loss: 0.225 \n",
      "epoch: 011, batch: 077, loss: 0.311 \n",
      "epoch: 011, batch: 078, loss: 0.581 \n",
      "epoch: 011, batch: 079, loss: 2.603 \n",
      "epoch: 011, batch: 080, loss: 0.108 \n",
      "epoch: 011, batch: 081, loss: 0.728 \n",
      "epoch: 011, batch: 082, loss: 0.192 \n",
      "epoch: 011, batch: 083, loss: 0.060 \n",
      "epoch: 011, batch: 084, loss: 0.201 \n",
      "epoch: 011, batch: 085, loss: 0.310 \n",
      "epoch: 011, batch: 086, loss: 0.348 \n",
      "epoch: 011, batch: 087, loss: 0.333 \n",
      "epoch: 011, batch: 088, loss: 0.410 \n",
      "epoch: 011, batch: 089, loss: 0.165 \n",
      "epoch: 011, batch: 090, loss: 0.559 \n",
      "epoch: 011, batch: 091, loss: 0.519 \n",
      "epoch: 011, batch: 092, loss: 0.132 \n",
      "epoch: 011, batch: 093, loss: 0.293 \n",
      "epoch: 011, batch: 094, loss: 0.169 \n",
      "epoch: 011, batch: 095, loss: 0.225 \n",
      "epoch: 011, batch: 096, loss: 0.130 \n",
      "epoch: 011, batch: 097, loss: 0.853 \n",
      "epoch: 011, batch: 098, loss: 1.004 \n",
      "epoch: 011, batch: 099, loss: 0.549 \n",
      "epoch: 011, batch: 100, loss: 0.801 \n",
      "epoch: 011, batch: 101, loss: 0.257 \n",
      "epoch: 011, batch: 102, loss: 0.114 \n",
      "epoch: 011, batch: 103, loss: 0.205 \n",
      "epoch: 011, batch: 104, loss: 0.202 \n",
      "epoch: 011, batch: 105, loss: 0.264 \n",
      "epoch: 011, batch: 106, loss: 0.271 \n",
      "epoch: 011, batch: 107, loss: 0.329 \n",
      "epoch: 011, batch: 108, loss: 0.188 \n",
      "epoch: 011, batch: 109, loss: 0.400 \n",
      "epoch: 011, batch: 110, loss: 0.343 \n",
      "epoch: 011, batch: 111, loss: 0.309 \n",
      "epoch: 011, batch: 112, loss: 0.189 \n",
      "epoch: 011, batch: 113, loss: 0.936 \n",
      "epoch: 011, batch: 114, loss: 0.384 \n",
      "epoch: 011, batch: 115, loss: 0.127 \n",
      "epoch: 011, batch: 116, loss: 0.175 \n",
      "epoch: 011, batch: 117, loss: 0.998 \n",
      "epoch: 011, batch: 118, loss: 0.355 \n",
      "epoch: 011, batch: 119, loss: 0.190 \n",
      "epoch: 011, batch: 120, loss: 0.177 \n",
      "epoch: 011, batch: 121, loss: 0.401 \n",
      "epoch: 011, batch: 122, loss: 0.258 \n",
      "epoch: 011, batch: 123, loss: 0.196 \n",
      "epoch: 011, batch: 124, loss: 0.698 \n",
      "epoch: 011, batch: 125, loss: 0.585 \n",
      "epoch: 011, batch: 126, loss: 0.342 \n",
      "epoch: 011 ------------------------------------------------\n",
      "\n",
      "[train] loss: 44.028\n",
      "\n",
      "\n",
      "[train] accuracy: 91.229%\n",
      "\n",
      "[train] bulls_recall: 49.198%\n",
      "\n",
      "[train] bulls_precision: 28.439%\n",
      "\n",
      "[train] no_bulls_recall: 93.452%\n",
      "\n",
      "[train] no_bulls_precision: 97.205%\n",
      "\n",
      "[validation] accuracy: 38.800%\n",
      "\n",
      "[validation] bulls_recall: 57.458%\n",
      "\n",
      "[validation] bulls_precision: 2.668%\n",
      "\n",
      "[validation] no_bulls_recall: 38.250%\n",
      "\n",
      "[validation] no_bulls_precision: 96.828%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.47854239706888346\n",
      "\n",
      "epoch: 012, batch: 001, loss: 0.677 \n",
      "epoch: 012, batch: 002, loss: 0.187 \n",
      "epoch: 012, batch: 003, loss: 0.186 \n",
      "epoch: 012, batch: 004, loss: 0.208 \n",
      "epoch: 012, batch: 005, loss: 0.107 \n",
      "epoch: 012, batch: 006, loss: 0.782 \n",
      "epoch: 012, batch: 007, loss: 0.380 \n",
      "epoch: 012, batch: 008, loss: 0.371 \n",
      "epoch: 012, batch: 009, loss: 0.261 \n",
      "epoch: 012, batch: 010, loss: 0.400 \n",
      "epoch: 012, batch: 011, loss: 0.163 \n",
      "epoch: 012, batch: 012, loss: 0.841 \n",
      "epoch: 012, batch: 013, loss: 0.244 \n",
      "epoch: 012, batch: 014, loss: 0.420 \n",
      "epoch: 012, batch: 015, loss: 0.165 \n",
      "epoch: 012, batch: 016, loss: 0.075 \n",
      "epoch: 012, batch: 017, loss: 0.381 \n",
      "epoch: 012, batch: 018, loss: 0.476 \n",
      "epoch: 012, batch: 019, loss: 0.164 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 012, batch: 020, loss: 1.526 \n",
      "epoch: 012, batch: 021, loss: 0.377 \n",
      "epoch: 012, batch: 022, loss: 0.192 \n",
      "epoch: 012, batch: 023, loss: 0.087 \n",
      "epoch: 012, batch: 024, loss: 0.082 \n",
      "epoch: 012, batch: 025, loss: 0.088 \n",
      "epoch: 012, batch: 026, loss: 0.171 \n",
      "epoch: 012, batch: 027, loss: 0.657 \n",
      "epoch: 012, batch: 028, loss: 0.096 \n",
      "epoch: 012, batch: 029, loss: 0.055 \n",
      "epoch: 012, batch: 030, loss: 0.451 \n",
      "epoch: 012, batch: 031, loss: 1.009 \n",
      "epoch: 012, batch: 032, loss: 0.273 \n",
      "epoch: 012, batch: 033, loss: 0.383 \n",
      "epoch: 012, batch: 034, loss: 0.333 \n",
      "epoch: 012, batch: 035, loss: 0.237 \n",
      "epoch: 012, batch: 036, loss: 0.375 \n",
      "epoch: 012, batch: 037, loss: 0.686 \n",
      "epoch: 012, batch: 038, loss: 0.088 \n",
      "epoch: 012, batch: 039, loss: 0.069 \n",
      "epoch: 012, batch: 040, loss: 0.038 \n",
      "epoch: 012, batch: 041, loss: 0.096 \n",
      "epoch: 012, batch: 042, loss: 0.087 \n",
      "epoch: 012, batch: 043, loss: 0.368 \n",
      "epoch: 012, batch: 044, loss: 0.164 \n",
      "epoch: 012, batch: 045, loss: 0.669 \n",
      "epoch: 012, batch: 046, loss: 1.166 \n",
      "epoch: 012, batch: 047, loss: 0.902 \n",
      "epoch: 012, batch: 048, loss: 0.122 \n",
      "epoch: 012, batch: 049, loss: 0.149 \n",
      "epoch: 012, batch: 050, loss: 0.203 \n",
      "epoch: 012, batch: 051, loss: 0.348 \n",
      "epoch: 012, batch: 052, loss: 0.295 \n",
      "epoch: 012, batch: 053, loss: 0.280 \n",
      "epoch: 012, batch: 054, loss: 0.463 \n",
      "epoch: 012, batch: 055, loss: 0.396 \n",
      "epoch: 012, batch: 056, loss: 0.198 \n",
      "epoch: 012, batch: 057, loss: 0.823 \n",
      "epoch: 012, batch: 058, loss: 0.014 \n",
      "epoch: 012, batch: 059, loss: 0.175 \n",
      "epoch: 012, batch: 060, loss: 0.301 \n",
      "epoch: 012, batch: 061, loss: 0.051 \n",
      "epoch: 012, batch: 062, loss: 0.024 \n",
      "epoch: 012, batch: 063, loss: 0.018 \n",
      "epoch: 012, batch: 064, loss: 0.033 \n",
      "epoch: 012, batch: 065, loss: 0.096 \n",
      "epoch: 012, batch: 066, loss: 0.210 \n",
      "epoch: 012, batch: 067, loss: 0.804 \n",
      "epoch: 012, batch: 068, loss: 0.003 \n",
      "epoch: 012, batch: 069, loss: 0.120 \n",
      "epoch: 012, batch: 070, loss: 0.103 \n",
      "epoch: 012, batch: 071, loss: 0.179 \n",
      "epoch: 012, batch: 072, loss: 0.160 \n",
      "epoch: 012, batch: 073, loss: 0.074 \n",
      "epoch: 012, batch: 074, loss: 0.043 \n",
      "epoch: 012, batch: 075, loss: 0.721 \n",
      "epoch: 012, batch: 076, loss: 0.118 \n",
      "epoch: 012, batch: 077, loss: 0.223 \n",
      "epoch: 012, batch: 078, loss: 0.077 \n",
      "epoch: 012, batch: 079, loss: 0.058 \n",
      "epoch: 012, batch: 080, loss: 0.188 \n",
      "epoch: 012, batch: 081, loss: 0.237 \n",
      "epoch: 012, batch: 082, loss: 0.069 \n",
      "epoch: 012, batch: 083, loss: 0.039 \n",
      "epoch: 012, batch: 084, loss: 0.335 \n",
      "epoch: 012, batch: 085, loss: 0.329 \n",
      "epoch: 012, batch: 086, loss: 0.311 \n",
      "epoch: 012, batch: 087, loss: 0.178 \n",
      "epoch: 012, batch: 088, loss: 0.036 \n",
      "epoch: 012, batch: 089, loss: 0.196 \n",
      "epoch: 012, batch: 090, loss: 0.317 \n",
      "epoch: 012, batch: 091, loss: 0.321 \n",
      "epoch: 012, batch: 092, loss: 0.250 \n",
      "epoch: 012, batch: 093, loss: 0.174 \n",
      "epoch: 012, batch: 094, loss: 0.799 \n",
      "epoch: 012, batch: 095, loss: 0.208 \n",
      "epoch: 012, batch: 096, loss: 0.119 \n",
      "epoch: 012, batch: 097, loss: 0.139 \n",
      "epoch: 012, batch: 098, loss: 0.904 \n",
      "epoch: 012, batch: 099, loss: 0.070 \n",
      "epoch: 012, batch: 100, loss: 0.438 \n",
      "epoch: 012, batch: 101, loss: 0.070 \n",
      "epoch: 012, batch: 102, loss: 0.313 \n",
      "epoch: 012, batch: 103, loss: 0.136 \n",
      "epoch: 012, batch: 104, loss: 0.832 \n",
      "epoch: 012, batch: 105, loss: 0.077 \n",
      "epoch: 012, batch: 106, loss: 0.253 \n",
      "epoch: 012, batch: 107, loss: 0.600 \n",
      "epoch: 012, batch: 108, loss: 0.328 \n",
      "epoch: 012, batch: 109, loss: 0.125 \n",
      "epoch: 012, batch: 110, loss: 0.485 \n",
      "epoch: 012, batch: 111, loss: 0.401 \n",
      "epoch: 012, batch: 112, loss: 0.382 \n",
      "epoch: 012, batch: 113, loss: 0.554 \n",
      "epoch: 012, batch: 114, loss: 0.206 \n",
      "epoch: 012, batch: 115, loss: 0.058 \n",
      "epoch: 012, batch: 116, loss: 0.783 \n",
      "epoch: 012, batch: 117, loss: 0.193 \n",
      "epoch: 012, batch: 118, loss: 0.545 \n",
      "epoch: 012, batch: 119, loss: 0.181 \n",
      "epoch: 012, batch: 120, loss: 0.073 \n",
      "epoch: 012, batch: 121, loss: 0.164 \n",
      "epoch: 012, batch: 122, loss: 0.242 \n",
      "epoch: 012, batch: 123, loss: 0.027 \n",
      "epoch: 012, batch: 124, loss: 0.318 \n",
      "epoch: 012, batch: 125, loss: 0.224 \n",
      "epoch: 012, batch: 126, loss: 0.122 \n",
      "epoch: 012 ------------------------------------------------\n",
      "\n",
      "[train] loss: 41.163\n",
      "\n",
      "\n",
      "[train] accuracy: 94.097%\n",
      "\n",
      "[train] bulls_recall: 53.883%\n",
      "\n",
      "[train] bulls_precision: 43.011%\n",
      "\n",
      "[train] no_bulls_recall: 96.224%\n",
      "\n",
      "[train] no_bulls_precision: 97.528%\n",
      "\n",
      "[validation] accuracy: 41.918%\n",
      "\n",
      "[validation] bulls_recall: 89.546%\n",
      "\n",
      "[validation] bulls_precision: 4.246%\n",
      "\n",
      "[validation] no_bulls_recall: 40.514%\n",
      "\n",
      "[validation] no_bulls_precision: 99.246%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.6502993300435788\n",
      "\n",
      "epoch: 013, batch: 001, loss: 0.198 \n",
      "epoch: 013, batch: 002, loss: 0.651 \n",
      "epoch: 013, batch: 003, loss: 0.097 \n",
      "epoch: 013, batch: 004, loss: 0.156 \n",
      "epoch: 013, batch: 005, loss: 0.731 \n",
      "epoch: 013, batch: 006, loss: 0.349 \n",
      "epoch: 013, batch: 007, loss: 0.202 \n",
      "epoch: 013, batch: 008, loss: 0.161 \n",
      "epoch: 013, batch: 009, loss: 0.144 \n",
      "epoch: 013, batch: 010, loss: 0.021 \n",
      "epoch: 013, batch: 011, loss: 1.058 \n",
      "epoch: 013, batch: 012, loss: 0.081 \n",
      "epoch: 013, batch: 013, loss: 0.182 \n",
      "epoch: 013, batch: 014, loss: 0.070 \n",
      "epoch: 013, batch: 015, loss: 0.383 \n",
      "epoch: 013, batch: 016, loss: 0.697 \n",
      "epoch: 013, batch: 017, loss: 0.266 \n",
      "epoch: 013, batch: 018, loss: 0.132 \n",
      "epoch: 013, batch: 019, loss: 0.126 \n",
      "epoch: 013, batch: 020, loss: 0.113 \n",
      "epoch: 013, batch: 021, loss: 1.280 \n",
      "epoch: 013, batch: 022, loss: 1.414 \n",
      "epoch: 013, batch: 023, loss: 0.123 \n",
      "epoch: 013, batch: 024, loss: 0.261 \n",
      "epoch: 013, batch: 025, loss: 0.101 \n",
      "epoch: 013, batch: 026, loss: 0.547 \n",
      "epoch: 013, batch: 027, loss: 0.063 \n",
      "epoch: 013, batch: 028, loss: 0.204 \n",
      "epoch: 013, batch: 029, loss: 0.191 \n",
      "epoch: 013, batch: 030, loss: 0.192 \n",
      "epoch: 013, batch: 031, loss: 0.092 \n",
      "epoch: 013, batch: 032, loss: 0.253 \n",
      "epoch: 013, batch: 033, loss: 0.151 \n",
      "epoch: 013, batch: 034, loss: 0.100 \n",
      "epoch: 013, batch: 035, loss: 0.176 \n",
      "epoch: 013, batch: 036, loss: 0.636 \n",
      "epoch: 013, batch: 037, loss: 0.129 \n",
      "epoch: 013, batch: 038, loss: 0.180 \n",
      "epoch: 013, batch: 039, loss: 0.297 \n",
      "epoch: 013, batch: 040, loss: 0.150 \n",
      "epoch: 013, batch: 041, loss: 0.150 \n",
      "epoch: 013, batch: 042, loss: 0.331 \n",
      "epoch: 013, batch: 043, loss: 0.042 \n",
      "epoch: 013, batch: 044, loss: 0.139 \n",
      "epoch: 013, batch: 045, loss: 0.108 \n",
      "epoch: 013, batch: 046, loss: 0.138 \n",
      "epoch: 013, batch: 047, loss: 0.233 \n",
      "epoch: 013, batch: 048, loss: 0.070 \n",
      "epoch: 013, batch: 049, loss: 0.148 \n",
      "epoch: 013, batch: 050, loss: 0.235 \n",
      "epoch: 013, batch: 051, loss: 0.172 \n",
      "epoch: 013, batch: 052, loss: 1.290 \n",
      "epoch: 013, batch: 053, loss: 0.176 \n",
      "epoch: 013, batch: 054, loss: 0.435 \n",
      "epoch: 013, batch: 055, loss: 0.045 \n",
      "epoch: 013, batch: 056, loss: 0.179 \n",
      "epoch: 013, batch: 057, loss: 2.194 \n",
      "epoch: 013, batch: 058, loss: 0.086 \n",
      "epoch: 013, batch: 059, loss: 0.087 \n",
      "epoch: 013, batch: 060, loss: 0.194 \n",
      "epoch: 013, batch: 061, loss: 0.274 \n",
      "epoch: 013, batch: 062, loss: 0.000 \n",
      "epoch: 013, batch: 063, loss: 0.081 \n",
      "epoch: 013, batch: 064, loss: 0.141 \n",
      "epoch: 013, batch: 065, loss: 1.272 \n",
      "epoch: 013, batch: 066, loss: 0.154 \n",
      "epoch: 013, batch: 067, loss: 0.053 \n",
      "epoch: 013, batch: 068, loss: 0.156 \n",
      "epoch: 013, batch: 069, loss: 0.176 \n",
      "epoch: 013, batch: 070, loss: 0.504 \n",
      "epoch: 013, batch: 071, loss: 0.170 \n",
      "epoch: 013, batch: 072, loss: 0.445 \n",
      "epoch: 013, batch: 073, loss: 0.586 \n",
      "epoch: 013, batch: 074, loss: 2.719 \n",
      "epoch: 013, batch: 075, loss: 0.166 \n",
      "epoch: 013, batch: 076, loss: 0.184 \n",
      "epoch: 013, batch: 077, loss: 0.392 \n",
      "epoch: 013, batch: 078, loss: 0.268 \n",
      "epoch: 013, batch: 079, loss: 0.996 \n",
      "epoch: 013, batch: 080, loss: 0.295 \n",
      "epoch: 013, batch: 081, loss: 0.510 \n",
      "epoch: 013, batch: 082, loss: 0.472 \n",
      "epoch: 013, batch: 083, loss: 1.325 \n",
      "epoch: 013, batch: 084, loss: 0.158 \n",
      "epoch: 013, batch: 085, loss: 0.488 \n",
      "epoch: 013, batch: 086, loss: 0.135 \n",
      "epoch: 013, batch: 087, loss: 0.395 \n",
      "epoch: 013, batch: 088, loss: 0.104 \n",
      "epoch: 013, batch: 089, loss: 0.137 \n",
      "epoch: 013, batch: 090, loss: 0.343 \n",
      "epoch: 013, batch: 091, loss: 0.124 \n",
      "epoch: 013, batch: 092, loss: 0.111 \n",
      "epoch: 013, batch: 093, loss: 0.042 \n",
      "epoch: 013, batch: 094, loss: 0.117 \n",
      "epoch: 013, batch: 095, loss: 0.681 \n",
      "epoch: 013, batch: 096, loss: 0.020 \n",
      "epoch: 013, batch: 097, loss: 0.226 \n",
      "epoch: 013, batch: 098, loss: 0.075 \n",
      "epoch: 013, batch: 099, loss: 0.122 \n",
      "epoch: 013, batch: 100, loss: 0.177 \n",
      "epoch: 013, batch: 101, loss: 0.191 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 013, batch: 102, loss: 0.805 \n",
      "epoch: 013, batch: 103, loss: 0.301 \n",
      "epoch: 013, batch: 104, loss: 0.062 \n",
      "epoch: 013, batch: 105, loss: 0.142 \n",
      "epoch: 013, batch: 106, loss: 3.803 \n",
      "epoch: 013, batch: 107, loss: 0.176 \n",
      "epoch: 013, batch: 108, loss: 0.098 \n",
      "epoch: 013, batch: 109, loss: 0.399 \n",
      "epoch: 013, batch: 110, loss: 0.153 \n",
      "epoch: 013, batch: 111, loss: 0.760 \n",
      "epoch: 013, batch: 112, loss: 0.325 \n",
      "epoch: 013, batch: 113, loss: 0.271 \n",
      "epoch: 013, batch: 114, loss: 0.309 \n",
      "epoch: 013, batch: 115, loss: 0.207 \n",
      "epoch: 013, batch: 116, loss: 1.079 \n",
      "epoch: 013, batch: 117, loss: 0.556 \n",
      "epoch: 013, batch: 118, loss: 0.107 \n",
      "epoch: 013, batch: 119, loss: 0.125 \n",
      "epoch: 013, batch: 120, loss: 0.088 \n",
      "epoch: 013, batch: 121, loss: 0.064 \n",
      "epoch: 013, batch: 122, loss: 0.811 \n",
      "epoch: 013, batch: 123, loss: 0.080 \n",
      "epoch: 013, batch: 124, loss: 0.150 \n",
      "epoch: 013, batch: 125, loss: 0.523 \n",
      "epoch: 013, batch: 126, loss: 0.064 \n",
      "epoch: 013 ------------------------------------------------\n",
      "\n",
      "[train] loss: 38.919\n",
      "\n",
      "\n",
      "[train] accuracy: 95.224%\n",
      "\n",
      "[train] bulls_recall: 62.391%\n",
      "\n",
      "[train] bulls_precision: 52.057%\n",
      "\n",
      "[train] no_bulls_recall: 96.961%\n",
      "\n",
      "[train] no_bulls_precision: 97.990%\n",
      "\n",
      "[validation] accuracy: 54.047%\n",
      "\n",
      "[validation] bulls_recall: 76.860%\n",
      "\n",
      "[validation] bulls_precision: 4.631%\n",
      "\n",
      "[validation] no_bulls_recall: 53.374%\n",
      "\n",
      "[validation] no_bulls_precision: 98.739%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.6511709324340875\n",
      "\n",
      "epoch: 014, batch: 001, loss: 1.455 \n",
      "epoch: 014, batch: 002, loss: 0.195 \n",
      "epoch: 014, batch: 003, loss: 0.207 \n",
      "epoch: 014, batch: 004, loss: 0.075 \n",
      "epoch: 014, batch: 005, loss: 0.215 \n",
      "epoch: 014, batch: 006, loss: 0.215 \n",
      "epoch: 014, batch: 007, loss: 0.196 \n",
      "epoch: 014, batch: 008, loss: 0.128 \n",
      "epoch: 014, batch: 009, loss: 0.565 \n",
      "epoch: 014, batch: 010, loss: 0.181 \n",
      "epoch: 014, batch: 011, loss: 0.218 \n",
      "epoch: 014, batch: 012, loss: 0.145 \n",
      "epoch: 014, batch: 013, loss: 0.088 \n",
      "epoch: 014, batch: 014, loss: 0.091 \n",
      "epoch: 014, batch: 015, loss: 0.440 \n",
      "epoch: 014, batch: 016, loss: 0.106 \n",
      "epoch: 014, batch: 017, loss: 0.079 \n",
      "epoch: 014, batch: 018, loss: 0.081 \n",
      "epoch: 014, batch: 019, loss: 0.185 \n",
      "epoch: 014, batch: 020, loss: 0.030 \n",
      "epoch: 014, batch: 021, loss: 0.134 \n",
      "epoch: 014, batch: 022, loss: 0.044 \n",
      "epoch: 014, batch: 023, loss: 0.071 \n",
      "epoch: 014, batch: 024, loss: 0.173 \n",
      "epoch: 014, batch: 025, loss: 1.356 \n",
      "epoch: 014, batch: 026, loss: 0.162 \n",
      "epoch: 014, batch: 027, loss: 0.099 \n",
      "epoch: 014, batch: 028, loss: 0.226 \n",
      "epoch: 014, batch: 029, loss: 0.540 \n",
      "epoch: 014, batch: 030, loss: 0.648 \n",
      "epoch: 014, batch: 031, loss: 0.043 \n",
      "epoch: 014, batch: 032, loss: 0.160 \n",
      "epoch: 014, batch: 033, loss: 0.508 \n",
      "epoch: 014, batch: 034, loss: 1.074 \n",
      "epoch: 014, batch: 035, loss: 0.094 \n",
      "epoch: 014, batch: 036, loss: 0.022 \n",
      "epoch: 014, batch: 037, loss: 0.115 \n",
      "epoch: 014, batch: 038, loss: 0.092 \n",
      "epoch: 014, batch: 039, loss: 0.172 \n",
      "epoch: 014, batch: 040, loss: 0.298 \n",
      "epoch: 014, batch: 041, loss: 0.078 \n",
      "epoch: 014, batch: 042, loss: 0.085 \n",
      "epoch: 014, batch: 043, loss: 0.325 \n",
      "epoch: 014, batch: 044, loss: 0.048 \n",
      "epoch: 014, batch: 045, loss: 0.217 \n",
      "epoch: 014, batch: 046, loss: 0.160 \n",
      "epoch: 014, batch: 047, loss: 0.361 \n",
      "epoch: 014, batch: 048, loss: 0.079 \n",
      "epoch: 014, batch: 049, loss: 0.322 \n",
      "epoch: 014, batch: 050, loss: 1.083 \n",
      "epoch: 014, batch: 051, loss: 0.096 \n",
      "epoch: 014, batch: 052, loss: 0.093 \n",
      "epoch: 014, batch: 053, loss: 0.810 \n",
      "epoch: 014, batch: 054, loss: 0.077 \n",
      "epoch: 014, batch: 055, loss: 0.624 \n",
      "epoch: 014, batch: 056, loss: 1.085 \n",
      "epoch: 014, batch: 057, loss: 0.085 \n",
      "epoch: 014, batch: 058, loss: 0.378 \n",
      "epoch: 014, batch: 059, loss: 0.377 \n",
      "epoch: 014, batch: 060, loss: 0.126 \n",
      "epoch: 014, batch: 061, loss: 0.566 \n",
      "epoch: 014, batch: 062, loss: 0.714 \n",
      "epoch: 014, batch: 063, loss: 0.323 \n",
      "epoch: 014, batch: 064, loss: 0.281 \n",
      "epoch: 014, batch: 065, loss: 0.332 \n",
      "epoch: 014, batch: 066, loss: 0.216 \n",
      "epoch: 014, batch: 067, loss: 0.303 \n",
      "epoch: 014, batch: 068, loss: 0.132 \n",
      "epoch: 014, batch: 069, loss: 0.145 \n",
      "epoch: 014, batch: 070, loss: 0.029 \n",
      "epoch: 014, batch: 071, loss: 0.192 \n",
      "epoch: 014, batch: 072, loss: 0.347 \n",
      "epoch: 014, batch: 073, loss: 0.058 \n",
      "epoch: 014, batch: 074, loss: 0.067 \n",
      "epoch: 014, batch: 075, loss: 0.590 \n",
      "epoch: 014, batch: 076, loss: 0.152 \n",
      "epoch: 014, batch: 077, loss: 0.076 \n",
      "epoch: 014, batch: 078, loss: 0.233 \n",
      "epoch: 014, batch: 079, loss: 0.044 \n",
      "epoch: 014, batch: 080, loss: 0.067 \n",
      "epoch: 014, batch: 081, loss: 0.235 \n",
      "epoch: 014, batch: 082, loss: 0.126 \n",
      "epoch: 014, batch: 083, loss: 0.059 \n",
      "epoch: 014, batch: 084, loss: 0.602 \n",
      "epoch: 014, batch: 085, loss: 0.088 \n",
      "epoch: 014, batch: 086, loss: 0.233 \n",
      "epoch: 014, batch: 087, loss: 0.304 \n",
      "epoch: 014, batch: 088, loss: 0.359 \n",
      "epoch: 014, batch: 089, loss: 0.120 \n",
      "epoch: 014, batch: 090, loss: 0.753 \n",
      "epoch: 014, batch: 091, loss: 0.134 \n",
      "epoch: 014, batch: 092, loss: 0.068 \n",
      "epoch: 014, batch: 093, loss: 0.150 \n",
      "epoch: 014, batch: 094, loss: 0.147 \n",
      "epoch: 014, batch: 095, loss: 0.184 \n",
      "epoch: 014, batch: 096, loss: 0.239 \n",
      "epoch: 014, batch: 097, loss: 0.297 \n",
      "epoch: 014, batch: 098, loss: 0.381 \n",
      "epoch: 014, batch: 099, loss: 0.450 \n",
      "epoch: 014, batch: 100, loss: 0.054 \n",
      "epoch: 014, batch: 101, loss: 0.270 \n",
      "epoch: 014, batch: 102, loss: 0.092 \n",
      "epoch: 014, batch: 103, loss: 0.166 \n",
      "epoch: 014, batch: 104, loss: 0.157 \n",
      "epoch: 014, batch: 105, loss: 0.440 \n",
      "epoch: 014, batch: 106, loss: 0.188 \n",
      "epoch: 014, batch: 107, loss: 0.528 \n",
      "epoch: 014, batch: 108, loss: 0.050 \n",
      "epoch: 014, batch: 109, loss: 0.572 \n",
      "epoch: 014, batch: 110, loss: 0.057 \n",
      "epoch: 014, batch: 111, loss: 3.344 \n",
      "epoch: 014, batch: 112, loss: 0.133 \n",
      "epoch: 014, batch: 113, loss: 0.193 \n",
      "epoch: 014, batch: 114, loss: 0.178 \n",
      "epoch: 014, batch: 115, loss: 0.194 \n",
      "epoch: 014, batch: 116, loss: 0.043 \n",
      "epoch: 014, batch: 117, loss: 0.069 \n",
      "epoch: 014, batch: 118, loss: 0.375 \n",
      "epoch: 014, batch: 119, loss: 0.824 \n",
      "epoch: 014, batch: 120, loss: 0.451 \n",
      "epoch: 014, batch: 121, loss: 0.235 \n",
      "epoch: 014, batch: 122, loss: 0.053 \n",
      "epoch: 014, batch: 123, loss: 0.288 \n",
      "epoch: 014, batch: 124, loss: 0.607 \n",
      "epoch: 014, batch: 125, loss: 0.167 \n",
      "epoch: 014, batch: 126, loss: 0.060 \n",
      "epoch: 014 ------------------------------------------------\n",
      "\n",
      "[train] loss: 36.701\n",
      "\n",
      "\n",
      "[train] accuracy: 93.608%\n",
      "\n",
      "[train] bulls_recall: 58.199%\n",
      "\n",
      "[train] bulls_precision: 40.515%\n",
      "\n",
      "[train] no_bulls_recall: 95.480%\n",
      "\n",
      "[train] no_bulls_precision: 97.737%\n",
      "\n",
      "[validation] accuracy: 76.361%\n",
      "\n",
      "[validation] bulls_recall: 97.754%\n",
      "\n",
      "[validation] bulls_precision: 10.607%\n",
      "\n",
      "[validation] no_bulls_recall: 75.731%\n",
      "\n",
      "[validation] no_bulls_precision: 99.913%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.8674233117981747\n",
      "\n",
      "\n",
      "Final nb epochs : 4\n",
      "Best validation score (= best bulls_fscore) : 0.8688461821138433\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows : real (no_bulls and bulls)  ;  columns : predicted (same) ): \n",
      "123  |  20\n",
      "563  |  3880\n",
      "\n",
      "[test] accuracy: 87.287%\n",
      "\n",
      "\n",
      "[test] bulls_recall: 86.008%\n",
      "\n",
      "[test] bulls_precision: 17.930%\n",
      "\n",
      "\n",
      "[test] no_bulls_recall: 87.328%\n",
      "\n",
      "[test] no_bulls_precision: 99.487%\n",
      "\n",
      "\n",
      "[test] scoring metric (average recall): 0.8666807832114536\n"
     ]
    }
   ],
   "source": [
    "model = model_alexnet(pretrained=True, freeze=False)\n",
    "name_model = \"model_alexnet_pretrained_batchsize128_update4\"\n",
    "batch_size = 128\n",
    "trainloader, validationloader, testloader, weight_bulls, weight_no_bulls = audio_importation(shuffle=True)\n",
    "epochs = 80\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "final_model = training(model=model, name_model=name_model, epochs=epochs, batch_size=batch_size, device=device)\n",
    "\n",
    "test(model=final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001, batch: 001, loss: 0.672 \n",
      "epoch: 001, batch: 002, loss: 0.479 \n",
      "epoch: 001, batch: 003, loss: 0.292 \n",
      "epoch: 001, batch: 004, loss: 0.336 \n",
      "epoch: 001, batch: 005, loss: 0.492 \n",
      "epoch: 001, batch: 006, loss: 0.422 \n",
      "epoch: 001, batch: 007, loss: 0.734 \n",
      "epoch: 001, batch: 008, loss: 0.477 \n",
      "epoch: 001, batch: 009, loss: 0.461 \n",
      "epoch: 001, batch: 010, loss: 0.402 \n",
      "epoch: 001, batch: 011, loss: 0.361 \n",
      "epoch: 001, batch: 012, loss: 0.426 \n",
      "epoch: 001, batch: 013, loss: 0.285 \n",
      "epoch: 001, batch: 014, loss: 0.376 \n",
      "epoch: 001, batch: 015, loss: 0.313 \n",
      "epoch: 001 ------------------------------------------------\n",
      "\n",
      "[train] loss: 9.561\n",
      "\n",
      "\n",
      "[train] accuracy: 94.165%\n",
      "\n",
      "[train] bulls_recall: 15.536%\n",
      "\n",
      "[train] bulls_precision: 32.897%\n",
      "\n",
      "[train] no_bulls_recall: 98.324%\n",
      "\n",
      "[train] no_bulls_precision: 95.654%\n",
      "\n",
      "[validation] accuracy: 91.053%\n",
      "\n",
      "[validation] bulls_recall: 9.701%\n",
      "\n",
      "[validation] bulls_precision: 4.180%\n",
      "\n",
      "[validation] no_bulls_recall: 93.449%\n",
      "\n",
      "[validation] no_bulls_precision: 97.232%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5157483643322027\n",
      "\n",
      "epoch: 002, batch: 001, loss: 0.817 \n",
      "epoch: 002, batch: 002, loss: 0.857 \n",
      "epoch: 002, batch: 003, loss: 0.400 \n",
      "epoch: 002, batch: 004, loss: 0.941 \n",
      "epoch: 002, batch: 005, loss: 0.858 \n",
      "epoch: 002, batch: 006, loss: 0.424 \n",
      "epoch: 002, batch: 007, loss: 0.222 \n",
      "epoch: 002, batch: 008, loss: 0.266 \n",
      "epoch: 002, batch: 009, loss: 0.343 \n",
      "epoch: 002, batch: 010, loss: 0.592 \n",
      "epoch: 002, batch: 011, loss: 0.369 \n",
      "epoch: 002, batch: 012, loss: 1.158 \n",
      "epoch: 002, batch: 013, loss: 0.643 \n",
      "epoch: 002, batch: 014, loss: 0.508 \n",
      "epoch: 002, batch: 015, loss: 0.466 \n",
      "epoch: 002 ------------------------------------------------\n",
      "\n",
      "[train] loss: 8.850\n",
      "\n",
      "\n",
      "[train] accuracy: 94.196%\n",
      "\n",
      "[train] bulls_recall: 19.728%\n",
      "\n",
      "[train] bulls_precision: 35.874%\n",
      "\n",
      "[train] no_bulls_recall: 98.135%\n",
      "\n",
      "[train] no_bulls_precision: 95.853%\n",
      "\n",
      "[validation] accuracy: 88.170%\n",
      "\n",
      "[validation] bulls_recall: 17.909%\n",
      "\n",
      "[validation] bulls_precision: 5.128%\n",
      "\n",
      "[validation] no_bulls_recall: 90.239%\n",
      "\n",
      "[validation] no_bulls_precision: 97.390%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5407426299507603\n",
      "\n",
      "epoch: 003, batch: 001, loss: 0.299 \n",
      "epoch: 003, batch: 002, loss: 0.317 \n",
      "epoch: 003, batch: 003, loss: 0.950 \n",
      "epoch: 003, batch: 004, loss: 0.449 \n",
      "epoch: 003, batch: 005, loss: 0.371 \n",
      "epoch: 003, batch: 006, loss: 0.814 \n",
      "epoch: 003, batch: 007, loss: 0.606 \n",
      "epoch: 003, batch: 008, loss: 0.834 \n",
      "epoch: 003, batch: 009, loss: 0.572 \n",
      "epoch: 003, batch: 010, loss: 0.860 \n",
      "epoch: 003, batch: 011, loss: 0.800 \n",
      "epoch: 003, batch: 012, loss: 0.348 \n",
      "epoch: 003, batch: 013, loss: 0.334 \n",
      "epoch: 003, batch: 014, loss: 0.487 \n",
      "epoch: 003, batch: 015, loss: 0.539 \n",
      "epoch: 003 ------------------------------------------------\n",
      "\n",
      "[train] loss: 8.277\n",
      "\n",
      "\n",
      "[train] accuracy: 94.803%\n",
      "\n",
      "[train] bulls_recall: 18.249%\n",
      "\n",
      "[train] bulls_precision: 45.678%\n",
      "\n",
      "[train] no_bulls_recall: 98.852%\n",
      "\n",
      "[train] no_bulls_precision: 95.809%\n",
      "\n",
      "[validation] accuracy: 89.750%\n",
      "\n",
      "[validation] bulls_recall: 11.939%\n",
      "\n",
      "[validation] bulls_precision: 4.233%\n",
      "\n",
      "[validation] no_bulls_recall: 92.042%\n",
      "\n",
      "[validation] no_bulls_precision: 97.259%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.519907061253959\n",
      "\n",
      "epoch: 004, batch: 001, loss: 0.214 \n",
      "epoch: 004, batch: 002, loss: 1.697 \n",
      "epoch: 004, batch: 003, loss: 0.286 \n",
      "epoch: 004, batch: 004, loss: 0.532 \n",
      "epoch: 004, batch: 005, loss: 0.184 \n",
      "epoch: 004, batch: 006, loss: 0.325 \n",
      "epoch: 004, batch: 007, loss: 0.631 \n",
      "epoch: 004, batch: 008, loss: 0.562 \n",
      "epoch: 004, batch: 009, loss: 0.375 \n",
      "epoch: 004, batch: 010, loss: 0.719 \n",
      "epoch: 004, batch: 011, loss: 0.194 \n",
      "epoch: 004, batch: 012, loss: 0.380 \n",
      "epoch: 004, batch: 013, loss: 0.390 \n",
      "epoch: 004, batch: 014, loss: 0.714 \n",
      "epoch: 004, batch: 015, loss: 1.270 \n",
      "epoch: 004 ------------------------------------------------\n",
      "\n",
      "[train] loss: 7.793\n",
      "\n",
      "\n",
      "[train] accuracy: 93.861%\n",
      "\n",
      "[train] bulls_recall: 24.291%\n",
      "\n",
      "[train] bulls_precision: 34.320%\n",
      "\n",
      "[train] no_bulls_recall: 97.541%\n",
      "\n",
      "[train] no_bulls_precision: 96.056%\n",
      "\n",
      "[validation] accuracy: 46.978%\n",
      "\n",
      "[validation] bulls_recall: 29.849%\n",
      "\n",
      "[validation] bulls_precision: 1.647%\n",
      "\n",
      "[validation] no_bulls_recall: 47.483%\n",
      "\n",
      "[validation] no_bulls_precision: 95.829%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.3866568883750926\n",
      "\n",
      "epoch: 005, batch: 001, loss: 0.335 \n",
      "epoch: 005, batch: 002, loss: 0.331 \n",
      "epoch: 005, batch: 003, loss: 0.343 \n",
      "epoch: 005, batch: 004, loss: 0.280 \n",
      "epoch: 005, batch: 005, loss: 1.343 \n",
      "epoch: 005, batch: 006, loss: 0.788 \n",
      "epoch: 005, batch: 007, loss: 0.601 \n",
      "epoch: 005, batch: 008, loss: 0.448 \n",
      "epoch: 005, batch: 009, loss: 0.477 \n",
      "epoch: 005, batch: 010, loss: 0.443 \n",
      "epoch: 005, batch: 011, loss: 0.220 \n",
      "epoch: 005, batch: 012, loss: 0.262 \n",
      "epoch: 005, batch: 013, loss: 0.289 \n",
      "epoch: 005, batch: 014, loss: 0.214 \n",
      "epoch: 005, batch: 015, loss: 0.289 \n",
      "epoch: 005 ------------------------------------------------\n",
      "\n",
      "[train] loss: 7.420\n",
      "\n",
      "\n",
      "[train] accuracy: 94.208%\n",
      "\n",
      "[train] bulls_recall: 25.400%\n",
      "\n",
      "[train] bulls_precision: 38.432%\n",
      "\n",
      "[train] no_bulls_recall: 97.848%\n",
      "\n",
      "[train] no_bulls_precision: 96.124%\n",
      "\n",
      "[validation] accuracy: 41.213%\n",
      "\n",
      "[validation] bulls_recall: 31.341%\n",
      "\n",
      "[validation] bulls_precision: 1.554%\n",
      "\n",
      "[validation] no_bulls_recall: 41.504%\n",
      "\n",
      "[validation] no_bulls_precision: 95.353%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.3642224031990563\n",
      "\n",
      "epoch: 006, batch: 001, loss: 0.337 \n",
      "epoch: 006, batch: 002, loss: 0.310 \n",
      "epoch: 006, batch: 003, loss: 0.554 \n",
      "epoch: 006, batch: 004, loss: 0.946 \n",
      "epoch: 006, batch: 005, loss: 0.288 \n",
      "epoch: 006, batch: 006, loss: 0.340 \n",
      "epoch: 006, batch: 007, loss: 0.890 \n",
      "epoch: 006, batch: 008, loss: 0.304 \n",
      "epoch: 006, batch: 009, loss: 0.712 \n",
      "epoch: 006, batch: 010, loss: 0.141 \n",
      "epoch: 006, batch: 011, loss: 0.464 \n",
      "epoch: 006, batch: 012, loss: 0.582 \n",
      "epoch: 006, batch: 013, loss: 0.353 \n",
      "epoch: 006, batch: 014, loss: 0.164 \n",
      "epoch: 006, batch: 015, loss: 0.224 \n",
      "epoch: 006 ------------------------------------------------\n",
      "\n",
      "[train] loss: 7.208\n",
      "\n",
      "\n",
      "[train] accuracy: 93.930%\n",
      "\n",
      "[train] bulls_recall: 25.031%\n",
      "\n",
      "[train] bulls_precision: 35.304%\n",
      "\n",
      "[train] no_bulls_recall: 97.574%\n",
      "\n",
      "[train] no_bulls_precision: 96.095%\n",
      "\n",
      "[validation] accuracy: 52.808%\n",
      "\n",
      "[validation] bulls_recall: 36.564%\n",
      "\n",
      "[validation] bulls_precision: 2.254%\n",
      "\n",
      "[validation] no_bulls_recall: 53.286%\n",
      "\n",
      "[validation] no_bulls_precision: 96.612%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.44925377465411054\n",
      "\n",
      "epoch: 007, batch: 001, loss: 0.851 \n",
      "epoch: 007, batch: 002, loss: 0.198 \n",
      "epoch: 007, batch: 003, loss: 0.966 \n",
      "epoch: 007, batch: 004, loss: 0.295 \n",
      "epoch: 007, batch: 005, loss: 0.195 \n",
      "epoch: 007, batch: 006, loss: 0.984 \n",
      "epoch: 007, batch: 007, loss: 0.280 \n",
      "epoch: 007, batch: 008, loss: 0.289 \n",
      "epoch: 007, batch: 009, loss: 0.139 \n",
      "epoch: 007, batch: 010, loss: 0.342 \n",
      "epoch: 007, batch: 011, loss: 0.626 \n",
      "epoch: 007, batch: 012, loss: 0.335 \n",
      "epoch: 007, batch: 013, loss: 0.904 \n",
      "epoch: 007, batch: 014, loss: 0.174 \n",
      "epoch: 007, batch: 015, loss: 0.055 \n",
      "epoch: 007 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.984\n",
      "\n",
      "\n",
      "[train] accuracy: 94.239%\n",
      "\n",
      "[train] bulls_recall: 24.907%\n",
      "\n",
      "[train] bulls_precision: 38.623%\n",
      "\n",
      "[train] no_bulls_recall: 97.906%\n",
      "\n",
      "[train] no_bulls_precision: 96.101%\n",
      "\n",
      "[validation] accuracy: 24.258%\n",
      "\n",
      "[validation] bulls_recall: 33.580%\n",
      "\n",
      "[validation] bulls_precision: 1.285%\n",
      "\n",
      "[validation] no_bulls_recall: 23.983%\n",
      "\n",
      "[validation] no_bulls_precision: 92.457%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.2878141196129658\n",
      "\n",
      "epoch: 008, batch: 001, loss: 0.484 \n",
      "epoch: 008, batch: 002, loss: 0.284 \n",
      "epoch: 008, batch: 003, loss: 0.941 \n",
      "epoch: 008, batch: 004, loss: 0.706 \n",
      "epoch: 008, batch: 005, loss: 0.737 \n",
      "epoch: 008, batch: 006, loss: 0.295 \n",
      "epoch: 008, batch: 007, loss: 0.267 \n",
      "epoch: 008, batch: 008, loss: 0.216 \n",
      "epoch: 008, batch: 009, loss: 0.125 \n",
      "epoch: 008, batch: 010, loss: 0.259 \n",
      "epoch: 008, batch: 011, loss: 0.023 \n",
      "epoch: 008, batch: 012, loss: 0.767 \n",
      "epoch: 008, batch: 013, loss: 0.132 \n",
      "epoch: 008, batch: 014, loss: 0.821 \n",
      "epoch: 008, batch: 015, loss: 0.389 \n",
      "epoch: 008 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.729\n",
      "\n",
      "\n",
      "[train] accuracy: 89.649%\n",
      "\n",
      "[train] bulls_recall: 44.636%\n",
      "\n",
      "[train] bulls_precision: 22.853%\n",
      "\n",
      "[train] no_bulls_recall: 92.030%\n",
      "\n",
      "[train] no_bulls_precision: 96.916%\n",
      "\n",
      "[validation] accuracy: 43.669%\n",
      "\n",
      "[validation] bulls_recall: 62.682%\n",
      "\n",
      "[validation] bulls_precision: 3.144%\n",
      "\n",
      "[validation] no_bulls_recall: 43.108%\n",
      "\n",
      "[validation] no_bulls_precision: 97.513%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5289508505703611\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 009, batch: 001, loss: 0.496 \n",
      "epoch: 009, batch: 002, loss: 0.177 \n",
      "epoch: 009, batch: 003, loss: 0.387 \n",
      "epoch: 009, batch: 004, loss: 0.296 \n",
      "epoch: 009, batch: 005, loss: 0.796 \n",
      "epoch: 009, batch: 006, loss: 0.450 \n",
      "epoch: 009, batch: 007, loss: 0.874 \n",
      "epoch: 009, batch: 008, loss: 0.227 \n",
      "epoch: 009, batch: 009, loss: 0.377 \n",
      "epoch: 009, batch: 010, loss: 0.343 \n",
      "epoch: 009, batch: 011, loss: 0.153 \n",
      "epoch: 009, batch: 012, loss: 0.233 \n",
      "epoch: 009, batch: 013, loss: 0.299 \n",
      "epoch: 009, batch: 014, loss: 0.081 \n",
      "epoch: 009, batch: 015, loss: 0.076 \n",
      "epoch: 009 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.393\n",
      "\n",
      "\n",
      "[train] accuracy: 94.363%\n",
      "\n",
      "[train] bulls_recall: 31.442%\n",
      "\n",
      "[train] bulls_precision: 41.871%\n",
      "\n",
      "[train] no_bulls_recall: 97.691%\n",
      "\n",
      "[train] no_bulls_precision: 96.421%\n",
      "\n",
      "[validation] accuracy: 31.476%\n",
      "\n",
      "[validation] bulls_recall: 49.250%\n",
      "\n",
      "[validation] bulls_precision: 2.058%\n",
      "\n",
      "[validation] no_bulls_recall: 30.952%\n",
      "\n",
      "[validation] no_bulls_precision: 95.392%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.4010092273810889\n",
      "\n",
      "epoch: 010, batch: 001, loss: 0.234 \n",
      "epoch: 010, batch: 002, loss: 0.228 \n",
      "epoch: 010, batch: 003, loss: 0.351 \n",
      "epoch: 010, batch: 004, loss: 0.317 \n",
      "epoch: 010, batch: 005, loss: 0.348 \n",
      "epoch: 010, batch: 006, loss: 0.703 \n",
      "epoch: 010, batch: 007, loss: 0.102 \n",
      "epoch: 010, batch: 008, loss: 0.501 \n",
      "epoch: 010, batch: 009, loss: 0.067 \n",
      "epoch: 010, batch: 010, loss: 0.230 \n",
      "epoch: 010, batch: 011, loss: 0.286 \n",
      "epoch: 010, batch: 012, loss: 0.108 \n",
      "epoch: 010, batch: 013, loss: 0.217 \n",
      "epoch: 010, batch: 014, loss: 0.332 \n",
      "epoch: 010, batch: 015, loss: 1.632 \n",
      "epoch: 010 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.332\n",
      "\n",
      "\n",
      "[train] accuracy: 89.860%\n",
      "\n",
      "[train] bulls_recall: 42.909%\n",
      "\n",
      "[train] bulls_precision: 22.865%\n",
      "\n",
      "[train] no_bulls_recall: 92.343%\n",
      "\n",
      "[train] no_bulls_precision: 96.833%\n",
      "\n",
      "[validation] accuracy: 50.438%\n",
      "\n",
      "[validation] bulls_recall: 79.845%\n",
      "\n",
      "[validation] bulls_precision: 4.456%\n",
      "\n",
      "[validation] no_bulls_recall: 49.571%\n",
      "\n",
      "[validation] no_bulls_precision: 98.816%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.6470800654495608\n",
      "\n",
      "epoch: 011, batch: 001, loss: 0.193 \n",
      "epoch: 011, batch: 002, loss: 0.471 \n",
      "epoch: 011, batch: 003, loss: 0.709 \n",
      "epoch: 011, batch: 004, loss: 0.106 \n",
      "epoch: 011, batch: 005, loss: 1.294 \n",
      "epoch: 011, batch: 006, loss: 0.761 \n",
      "epoch: 011, batch: 007, loss: 0.674 \n",
      "epoch: 011, batch: 008, loss: 0.113 \n",
      "epoch: 011, batch: 009, loss: 0.757 \n",
      "epoch: 011, batch: 010, loss: 0.150 \n",
      "epoch: 011, batch: 011, loss: 0.096 \n",
      "epoch: 011, batch: 012, loss: 0.185 \n",
      "epoch: 011, batch: 013, loss: 0.144 \n",
      "epoch: 011, batch: 014, loss: 0.690 \n",
      "epoch: 011, batch: 015, loss: 0.340 \n",
      "epoch: 011 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.285\n",
      "\n",
      "\n",
      "[train] accuracy: 92.499%\n",
      "\n",
      "[train] bulls_recall: 34.895%\n",
      "\n",
      "[train] bulls_precision: 29.296%\n",
      "\n",
      "[train] no_bulls_recall: 95.545%\n",
      "\n",
      "[train] no_bulls_precision: 96.521%\n",
      "\n",
      "[validation] accuracy: 67.927%\n",
      "\n",
      "[validation] bulls_recall: 52.981%\n",
      "\n",
      "[validation] bulls_precision: 4.702%\n",
      "\n",
      "[validation] no_bulls_recall: 68.367%\n",
      "\n",
      "[validation] no_bulls_precision: 98.014%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.6067382225851876\n",
      "\n",
      "epoch: 012, batch: 001, loss: 0.094 \n",
      "epoch: 012, batch: 002, loss: 0.493 \n",
      "epoch: 012, batch: 003, loss: 0.035 \n",
      "epoch: 012, batch: 004, loss: 0.499 \n",
      "epoch: 012, batch: 005, loss: 0.689 \n",
      "epoch: 012, batch: 006, loss: 0.393 \n",
      "epoch: 012, batch: 007, loss: 0.773 \n",
      "epoch: 012, batch: 008, loss: 0.410 \n",
      "epoch: 012, batch: 009, loss: 0.128 \n",
      "epoch: 012, batch: 010, loss: 0.255 \n",
      "epoch: 012, batch: 011, loss: 0.777 \n",
      "epoch: 012, batch: 012, loss: 0.267 \n",
      "epoch: 012, batch: 013, loss: 0.351 \n",
      "epoch: 012, batch: 014, loss: 0.234 \n",
      "epoch: 012, batch: 015, loss: 0.026 \n",
      "epoch: 012 ------------------------------------------------\n",
      "\n",
      "[train] loss: 6.072\n",
      "\n",
      "\n",
      "[train] accuracy: 94.753%\n",
      "\n",
      "[train] bulls_recall: 30.826%\n",
      "\n",
      "[train] bulls_precision: 46.641%\n",
      "\n",
      "[train] no_bulls_recall: 98.135%\n",
      "\n",
      "[train] no_bulls_precision: 96.406%\n",
      "\n",
      "[validation] accuracy: 22.998%\n",
      "\n",
      "[validation] bulls_recall: 40.296%\n",
      "\n",
      "[validation] bulls_precision: 1.508%\n",
      "\n",
      "[validation] no_bulls_recall: 22.488%\n",
      "\n",
      "[validation] no_bulls_precision: 92.746%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.3139195495091664\n",
      "\n",
      "epoch: 013, batch: 001, loss: 0.094 \n",
      "epoch: 013, batch: 002, loss: 0.369 \n",
      "epoch: 013, batch: 003, loss: 0.186 \n",
      "epoch: 013, batch: 004, loss: 0.181 \n",
      "epoch: 013, batch: 005, loss: 0.106 \n",
      "epoch: 013, batch: 006, loss: 0.149 \n",
      "epoch: 013, batch: 007, loss: 0.346 \n",
      "epoch: 013, batch: 008, loss: 0.782 \n",
      "epoch: 013, batch: 009, loss: 0.644 \n",
      "epoch: 013, batch: 010, loss: 0.094 \n",
      "epoch: 013, batch: 011, loss: 0.909 \n",
      "epoch: 013, batch: 012, loss: 0.230 \n",
      "epoch: 013, batch: 013, loss: 0.274 \n",
      "epoch: 013, batch: 014, loss: 0.415 \n",
      "epoch: 013, batch: 015, loss: 0.128 \n",
      "epoch: 013 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.937\n",
      "\n",
      "\n",
      "[train] accuracy: 95.305%\n",
      "\n",
      "[train] bulls_recall: 32.552%\n",
      "\n",
      "[train] bulls_precision: 55.578%\n",
      "\n",
      "[train] no_bulls_recall: 98.624%\n",
      "\n",
      "[train] no_bulls_precision: 96.509%\n",
      "\n",
      "[validation] accuracy: 33.269%\n",
      "\n",
      "[validation] bulls_recall: 44.773%\n",
      "\n",
      "[validation] bulls_precision: 1.929%\n",
      "\n",
      "[validation] no_bulls_recall: 32.930%\n",
      "\n",
      "[validation] no_bulls_precision: 95.292%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.38851510057809724\n",
      "\n",
      "epoch: 014, batch: 001, loss: 0.838 \n",
      "epoch: 014, batch: 002, loss: 0.528 \n",
      "epoch: 014, batch: 003, loss: 0.200 \n",
      "epoch: 014, batch: 004, loss: 0.205 \n",
      "epoch: 014, batch: 005, loss: 0.217 \n",
      "epoch: 014, batch: 006, loss: 0.384 \n",
      "epoch: 014, batch: 007, loss: 0.273 \n",
      "epoch: 014, batch: 008, loss: 0.067 \n",
      "epoch: 014, batch: 009, loss: 0.284 \n",
      "epoch: 014, batch: 010, loss: 0.553 \n",
      "epoch: 014, batch: 011, loss: 0.116 \n",
      "epoch: 014, batch: 012, loss: 0.403 \n",
      "epoch: 014, batch: 013, loss: 0.260 \n",
      "epoch: 014, batch: 014, loss: 1.346 \n",
      "epoch: 014, batch: 015, loss: 0.447 \n",
      "epoch: 014 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.771\n",
      "\n",
      "\n",
      "[train] accuracy: 93.056%\n",
      "\n",
      "[train] bulls_recall: 43.156%\n",
      "\n",
      "[train] bulls_precision: 34.653%\n",
      "\n",
      "[train] no_bulls_recall: 95.695%\n",
      "\n",
      "[train] no_bulls_precision: 96.954%\n",
      "\n",
      "[validation] accuracy: 38.586%\n",
      "\n",
      "[validation] bulls_recall: 76.114%\n",
      "\n",
      "[validation] bulls_precision: 3.462%\n",
      "\n",
      "[validation] no_bulls_recall: 37.481%\n",
      "\n",
      "[validation] no_bulls_precision: 98.157%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5679720273328283\n",
      "\n",
      "epoch: 015, batch: 001, loss: 0.226 \n",
      "epoch: 015, batch: 002, loss: 0.145 \n",
      "epoch: 015, batch: 003, loss: 0.049 \n",
      "epoch: 015, batch: 004, loss: 0.583 \n",
      "epoch: 015, batch: 005, loss: 0.247 \n",
      "epoch: 015, batch: 006, loss: 0.157 \n",
      "epoch: 015, batch: 007, loss: 1.223 \n",
      "epoch: 015, batch: 008, loss: 0.558 \n",
      "epoch: 015, batch: 009, loss: 0.691 \n",
      "epoch: 015, batch: 010, loss: 0.470 \n",
      "epoch: 015, batch: 011, loss: 0.384 \n",
      "epoch: 015, batch: 012, loss: 0.326 \n",
      "epoch: 015, batch: 013, loss: 0.209 \n",
      "epoch: 015, batch: 014, loss: 0.077 \n",
      "epoch: 015, batch: 015, loss: 0.137 \n",
      "epoch: 015 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.745\n",
      "\n",
      "\n",
      "[train] accuracy: 94.561%\n",
      "\n",
      "[train] bulls_recall: 30.949%\n",
      "\n",
      "[train] bulls_precision: 44.112%\n",
      "\n",
      "[train] no_bulls_recall: 97.926%\n",
      "\n",
      "[train] no_bulls_precision: 96.404%\n",
      "\n",
      "[validation] accuracy: 52.082%\n",
      "\n",
      "[validation] bulls_recall: 45.519%\n",
      "\n",
      "[validation] bulls_precision: 2.733%\n",
      "\n",
      "[validation] no_bulls_recall: 52.275%\n",
      "\n",
      "[validation] no_bulls_precision: 97.021%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.4889705076435201\n",
      "\n",
      "epoch: 016, batch: 001, loss: 0.178 \n",
      "epoch: 016, batch: 002, loss: 0.642 \n",
      "epoch: 016, batch: 003, loss: 0.145 \n",
      "epoch: 016, batch: 004, loss: 0.135 \n",
      "epoch: 016, batch: 005, loss: 0.351 \n",
      "epoch: 016, batch: 006, loss: 0.245 \n",
      "epoch: 016, batch: 007, loss: 0.683 \n",
      "epoch: 016, batch: 008, loss: 0.552 \n",
      "epoch: 016, batch: 009, loss: 0.192 \n",
      "epoch: 016, batch: 010, loss: 0.009 \n",
      "epoch: 016, batch: 011, loss: 0.911 \n",
      "epoch: 016, batch: 012, loss: 0.151 \n",
      "epoch: 016, batch: 013, loss: 0.518 \n",
      "epoch: 016, batch: 014, loss: 0.108 \n",
      "epoch: 016, batch: 015, loss: 0.523 \n",
      "epoch: 016 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.600\n",
      "\n",
      "\n",
      "[train] accuracy: 60.852%\n",
      "\n",
      "[train] bulls_recall: 95.437%\n",
      "\n",
      "[train] bulls_precision: 10.968%\n",
      "\n",
      "[train] no_bulls_recall: 59.023%\n",
      "\n",
      "[train] no_bulls_precision: 99.593%\n",
      "\n",
      "[validation] accuracy: 26.500%\n",
      "\n",
      "[validation] bulls_recall: 61.936%\n",
      "\n",
      "[validation] bulls_precision: 2.389%\n",
      "\n",
      "[validation] no_bulls_recall: 25.456%\n",
      "\n",
      "[validation] no_bulls_precision: 95.781%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.4369588234491458\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 017, batch: 001, loss: 0.410 \n",
      "epoch: 017, batch: 002, loss: 0.146 \n",
      "epoch: 017, batch: 003, loss: 0.953 \n",
      "epoch: 017, batch: 004, loss: 0.096 \n",
      "epoch: 017, batch: 005, loss: 0.128 \n",
      "epoch: 017, batch: 006, loss: 0.443 \n",
      "epoch: 017, batch: 007, loss: 0.290 \n",
      "epoch: 017, batch: 008, loss: 0.657 \n",
      "epoch: 017, batch: 009, loss: 0.497 \n",
      "epoch: 017, batch: 010, loss: 0.375 \n",
      "epoch: 017, batch: 011, loss: 0.150 \n",
      "epoch: 017, batch: 012, loss: 0.079 \n",
      "epoch: 017, batch: 013, loss: 0.561 \n",
      "epoch: 017, batch: 014, loss: 0.745 \n",
      "epoch: 017, batch: 015, loss: 0.311 \n",
      "epoch: 017 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.901\n",
      "\n",
      "\n",
      "[train] accuracy: 95.032%\n",
      "\n",
      "[train] bulls_recall: 28.236%\n",
      "\n",
      "[train] bulls_precision: 51.001%\n",
      "\n",
      "[train] no_bulls_recall: 98.565%\n",
      "\n",
      "[train] no_bulls_precision: 96.292%\n",
      "\n",
      "[validation] accuracy: 41.384%\n",
      "\n",
      "[validation] bulls_recall: 42.534%\n",
      "\n",
      "[validation] bulls_precision: 2.092%\n",
      "\n",
      "[validation] no_bulls_recall: 41.350%\n",
      "\n",
      "[validation] no_bulls_precision: 96.067%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.41941897771091546\n",
      "\n",
      "epoch: 018, batch: 001, loss: 0.158 \n",
      "epoch: 018, batch: 002, loss: 0.250 \n",
      "epoch: 018, batch: 003, loss: 0.170 \n",
      "epoch: 018, batch: 004, loss: 0.263 \n",
      "epoch: 018, batch: 005, loss: 0.293 \n",
      "epoch: 018, batch: 006, loss: 1.010 \n",
      "epoch: 018, batch: 007, loss: 0.175 \n",
      "epoch: 018, batch: 008, loss: 0.211 \n",
      "epoch: 018, batch: 009, loss: 0.083 \n",
      "epoch: 018, batch: 010, loss: 0.372 \n",
      "epoch: 018, batch: 011, loss: 0.548 \n",
      "epoch: 018, batch: 012, loss: 0.509 \n",
      "epoch: 018, batch: 013, loss: 0.211 \n",
      "epoch: 018, batch: 014, loss: 0.143 \n",
      "epoch: 018, batch: 015, loss: 0.133 \n",
      "epoch: 018 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.650\n",
      "\n",
      "\n",
      "[train] accuracy: 90.833%\n",
      "\n",
      "[train] bulls_recall: 46.855%\n",
      "\n",
      "[train] bulls_precision: 26.592%\n",
      "\n",
      "[train] no_bulls_recall: 93.158%\n",
      "\n",
      "[train] no_bulls_precision: 97.071%\n",
      "\n",
      "[validation] accuracy: 21.375%\n",
      "\n",
      "[validation] bulls_recall: 51.489%\n",
      "\n",
      "[validation] bulls_precision: 1.872%\n",
      "\n",
      "[validation] no_bulls_recall: 20.488%\n",
      "\n",
      "[validation] no_bulls_precision: 93.480%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.3598833459000718\n",
      "\n",
      "epoch: 019, batch: 001, loss: 0.652 \n",
      "epoch: 019, batch: 002, loss: 0.344 \n",
      "epoch: 019, batch: 003, loss: 1.369 \n",
      "epoch: 019, batch: 004, loss: 0.536 \n",
      "epoch: 019, batch: 005, loss: 0.155 \n",
      "epoch: 019, batch: 006, loss: 0.457 \n",
      "epoch: 019, batch: 007, loss: 0.099 \n",
      "epoch: 019, batch: 008, loss: 0.446 \n",
      "epoch: 019, batch: 009, loss: 0.143 \n",
      "epoch: 019, batch: 010, loss: 0.230 \n",
      "epoch: 019, batch: 011, loss: 0.044 \n",
      "epoch: 019, batch: 012, loss: 0.728 \n",
      "epoch: 019, batch: 013, loss: 0.552 \n",
      "epoch: 019, batch: 014, loss: 0.736 \n",
      "epoch: 019, batch: 015, loss: 0.232 \n",
      "epoch: 019 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.806\n",
      "\n",
      "\n",
      "[train] accuracy: 94.047%\n",
      "\n",
      "[train] bulls_recall: 32.429%\n",
      "\n",
      "[train] bulls_precision: 38.905%\n",
      "\n",
      "[train] no_bulls_recall: 97.306%\n",
      "\n",
      "[train] no_bulls_precision: 96.457%\n",
      "\n",
      "[validation] accuracy: 25.133%\n",
      "\n",
      "[validation] bulls_recall: 48.504%\n",
      "\n",
      "[validation] bulls_precision: 1.856%\n",
      "\n",
      "[validation] no_bulls_recall: 24.445%\n",
      "\n",
      "[validation] no_bulls_precision: 94.157%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.3647436110613447\n",
      "\n",
      "epoch: 020, batch: 001, loss: 0.147 \n",
      "epoch: 020, batch: 002, loss: 0.240 \n",
      "epoch: 020, batch: 003, loss: 0.714 \n",
      "epoch: 020, batch: 004, loss: 0.129 \n",
      "epoch: 020, batch: 005, loss: 0.568 \n",
      "epoch: 020, batch: 006, loss: 0.464 \n",
      "epoch: 020, batch: 007, loss: 0.373 \n",
      "epoch: 020, batch: 008, loss: 0.188 \n",
      "epoch: 020, batch: 009, loss: 0.089 \n",
      "epoch: 020, batch: 010, loss: 0.066 \n",
      "epoch: 020, batch: 011, loss: 0.211 \n",
      "epoch: 020, batch: 012, loss: 0.200 \n",
      "epoch: 020, batch: 013, loss: 0.008 \n",
      "epoch: 020, batch: 014, loss: 0.038 \n",
      "epoch: 020, batch: 015, loss: 0.509 \n",
      "epoch: 020 ------------------------------------------------\n",
      "\n",
      "[train] loss: 5.364\n",
      "\n",
      "\n",
      "[train] accuracy: 89.364%\n",
      "\n",
      "[train] bulls_recall: 55.363%\n",
      "\n",
      "[train] bulls_precision: 24.889%\n",
      "\n",
      "[train] no_bulls_recall: 91.163%\n",
      "\n",
      "[train] no_bulls_precision: 97.476%\n",
      "\n",
      "[validation] accuracy: 42.729%\n",
      "\n",
      "[validation] bulls_recall: 77.606%\n",
      "\n",
      "[validation] bulls_precision: 3.774%\n",
      "\n",
      "[validation] no_bulls_recall: 41.701%\n",
      "\n",
      "[validation] no_bulls_precision: 98.443%\n",
      "\n",
      "[validation] scoring metric (average recall): 0.5965376498725368\n",
      "\n",
      "\n",
      "Final nb epochs : 10\n",
      "Best validation score (= best bulls_fscore) : 0.6470800654495608\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows : real (no_bulls and bulls)  ;  columns : predicted (same) ): \n",
      "88  |  55\n",
      "589  |  3854\n",
      "\n",
      "[test] accuracy: 85.957%\n",
      "\n",
      "\n",
      "[test] bulls_recall: 61.534%\n",
      "\n",
      "[test] bulls_precision: 12.998%\n",
      "\n",
      "\n",
      "[test] no_bulls_recall: 86.743%\n",
      "\n",
      "[test] no_bulls_precision: 98.593%\n",
      "\n",
      "\n",
      "[test] scoring metric (average recall): 0.7413857737625724\n"
     ]
    }
   ],
   "source": [
    "model = model_alexnet(pretrained=True, freeze=False)\n",
    "name_model = \"model_alexnet_pretrained_batchsize1024_update4\"\n",
    "batch_size = 1024\n",
    "trainloader, validationloader, testloader, weight_bulls, weight_no_bulls = audio_importation(shuffle=True)\n",
    "epochs = 80\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "final_model = training(model=model, name_model=name_model, epochs=epochs, batch_size=batch_size, device=device)\n",
    "\n",
    "test(model=final_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
